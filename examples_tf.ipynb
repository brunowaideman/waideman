{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e9c7d6dfa360657e11777b74047b46cb835679c20e146a64b8c45f534af8e9d6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from configparser import ConfigParser\n",
    "from tqdm.auto import tqdm\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from utils import get_config, create_data_splits, OrderedDictWithDefaultList\n",
    "\n",
    "# Read config.ini file\n",
    "SETTINGS, COLOURS, EYETRACKER, TF = get_config(\"config.ini\")\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "source": [
    "# Read and process the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 15544/15544 [00:31<00:00, 486.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv(\"data/positions.csv\")\n",
    "    df[\"targets\"] = list(zip(df[\"x\"], df[\"y\"]))\n",
    "    df[\"filename\"] = df[\"id\"].astype(\"str\") + \".jpg\"\n",
    "    filenames = df[\"filename\"].tolist()\n",
    "\n",
    "    data = OrderedDictWithDefaultList()\n",
    "    data[\"targets\"] = np.array(df[\"targets\"].tolist(), dtype=\"float32\")\n",
    "    data[\"head_angle\"] = np.array(df[\"head_angle\"], dtype=\"float32\")\n",
    "    img_types = [\"face\", \"face_aligned\", \"l_eye\", \"r_eye\", \"head_pos\"]\n",
    "\n",
    "    for f in tqdm(filenames):\n",
    "        for img_type in img_types:\n",
    "            cur_img = load_img(\"data/{}/{}\".format(img_type, f))\n",
    "            data[img_type].append(img_to_array(cur_img))\n",
    "\n",
    "    for img_type in img_types:\n",
    "        data[img_type] = np.array(data[img_type], dtype=\"float32\") / 255.0\n",
    "\n",
    "    return create_data_splits(data)\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TF data pipeline test\n",
    "# def create_datasets(*desired_data):\n",
    "#     all_features = []\n",
    "#     data_list = list(desired_data)\n",
    "\n",
    "#     # Get xy targets\n",
    "#     df = pd.read_csv(\"data/positions.csv\", sep=\",\")\n",
    "#     xy_targets = list(zip(df[\"x\"], df[\"y\"]))\n",
    "#     d_targets = tf.data.Dataset.from_tensor_slices(xy_targets)\n",
    "#     print(\"targets : \", d_targets)\n",
    "\n",
    "#     # Get head_angle value if specified\n",
    "#     if \"head_angle\" in data_list:\n",
    "#         data_list.remove(\"head_angle\")\n",
    "#         d_head_angle = tf.data.Dataset.from_tensor_slices(\n",
    "#             df[\"head_angle\"].astype(np.float32)\n",
    "#         )\n",
    "#         all_features.append(d_head_angle)\n",
    "#         print(\"head_angle : \", d_head_angle)\n",
    "\n",
    "#     # Get requested images\n",
    "#     for d in data_list:\n",
    "#         img_list = tf.data.Dataset.list_files(\"data/{}/*\".format(d), shuffle=False)\n",
    "#         img_list = img_list.map(\n",
    "#             process_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "#         )\n",
    "#         all_features.append(img_list)\n",
    "#         print(d, \": \", img_list)\n",
    "\n",
    "#     # Combine inputs with targets ((a,b,c), (x,y))\n",
    "#     all_features = tf.data.Dataset.zip(tuple(all_features))\n",
    "#     ds = tf.data.Dataset.zip((all_features, d_targets))\n",
    "#     ds = ds.shuffle(1000, reshuffle_each_iteration=False)\n",
    "\n",
    "#     # Split into train/val/test (80/10/10)\n",
    "#     d_train, d_val, d_test = split_ds(ds)\n",
    "\n",
    "#     return {\n",
    "#         \"train\": optimize_ds(d_train),\n",
    "#         \"val\": optimize_ds(d_val),\n",
    "#         \"test\": optimize_ds(d_test),\n",
    "#     }\n",
    "\n",
    "\n",
    "# def process_image(path):\n",
    "#     img = tf.io.read_file(path)\n",
    "#     img = tf.image.decode_jpeg(img)\n",
    "#     img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "#     return img\n",
    "\n",
    "\n",
    "# def split_ds(ds, train=0.8, val=0.1, test=0.1):\n",
    "#     assert train + val + test == 1.0\n",
    "\n",
    "#     ds_size = tf.data.experimental.cardinality(ds).numpy()\n",
    "#     train_size = int(train * ds_size)\n",
    "#     val_size = int(val * ds_size)\n",
    "#     test_size = int(test * ds_size)\n",
    "\n",
    "#     d_train = ds.take(train_size)\n",
    "#     d_test = ds.skip(train_size)\n",
    "#     d_val = d_test.take(val_size)\n",
    "#     d_test = d_test.skip(val_size)\n",
    "\n",
    "#     return d_train, d_val, d_test\n",
    "\n",
    "\n",
    "# def optimize_ds(ds, batch_size=128):\n",
    "#     ds = ds.cache()\n",
    "#     ds = ds.shuffle(buffer_size=1000)\n",
    "#     ds = ds.batch(batch_size)\n",
    "#     ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#     return ds\n"
   ]
  },
  {
   "source": [
    "# Model experiments\n",
    "## Basic sequential model using a single face image"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "source": [
    "HP_OPTIMIZER = hp.HParam(\"optimizer\", hp.Discrete([\"adam\", \"rmsprop\", \"adagrad\"]))\n",
    "HP_CONV_KERNEL_SIZE = hp.HParam(\"conv_kernel_size\", hp.Discrete([3, 5, 7, 9]))\n",
    "HP_CONV_LAYERS = hp.HParam(\"conv_layers\", hp.IntInterval(1, 5))\n",
    "HP_DENSE_LAYERS = hp.HParam(\"dense_layers\", hp.IntInterval(1, 3))\n",
    "HP_DROPOUT = hp.HParam(\"dropout\", hp.RealInterval(0.1, 0.4))\n",
    "\n",
    "HPARAMS = [\n",
    "    HP_OPTIMIZER,\n",
    "    HP_CONV_KERNEL_SIZE,\n",
    "    HP_CONV_LAYERS,\n",
    "    HP_DENSE_LAYERS,\n",
    "    HP_DROPOUT,\n",
    "]\n",
    "\n",
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"epoch_loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"loss (val)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"batch_loss\",\n",
    "        group=\"train\",\n",
    "        display_name=\"loss (train)\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "def build_model(hparams, seed=87):\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    conv_filters = 8\n",
    "    model.add(layers.Conv2D(conv_filters, hparams[HP_CONV_KERNEL_SIZE], activation='relu', input_shape=(SETTINGS['image_size'], SETTINGS['image_size'], 3)))\n",
    "    model.add(layers.MaxPool2D(pool_size=2, padding=\"same\"))\n",
    "\n",
    "    for _ in range(hparams[HP_CONV_LAYERS]):\n",
    "        model.add(\n",
    "            layers.Conv2D(\n",
    "                filters=conv_filters,\n",
    "                kernel_size=hparams[HP_CONV_KERNEL_SIZE],\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.MaxPool2D(pool_size=2, padding=\"same\"))\n",
    "        conv_filters *= 2\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(hparams[HP_DROPOUT], seed=rng.random()))\n",
    "\n",
    "    # Add fully connected layers\n",
    "    dense_neurons = 32\n",
    "    for _ in range(hparams[HP_DENSE_LAYERS]):\n",
    "        model.add(layers.Dense(dense_neurons, activation=\"relu\"))\n",
    "        dense_neurons *= 2\n",
    "\n",
    "    # Add the final output layer\n",
    "    model.add(layers.Dense(2))\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=hparams[HP_OPTIMIZER],\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def run_single(data, base_logdir, session_id, hparams, best_loss):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = build_model(hparams=hparams, seed=session_id)\n",
    "\n",
    "    logdir = os.path.join(base_logdir, session_id)\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "        logdir,\n",
    "        histogram_freq=0,\n",
    "        update_freq=500,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    )\n",
    "    hparams_callback = hp.KerasCallback(logdir, hparams)\n",
    "\n",
    "    history = model.fit(\n",
    "        x=data['face']['train'],\n",
    "        y=data['targets']['train'],\n",
    "        epochs=2,\n",
    "        shuffle=False,\n",
    "        validation_data=(data['face']['val'], data['targets']['val']),\n",
    "        callbacks=[tb_callback, hparams_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    results = model.evaluate(x=data['face']['test'], y=data['targets']['test'], batch_size=None, verbose=0)\n",
    "    if results[1] < best_loss:\n",
    "        print(results)\n",
    "        print(\"Saving new model. Loss: {}\".format(round(results[1]*100, 3)))\n",
    "        model.save('models/eyetracking_model.h5')\n",
    "        return results[1]\n",
    "    else:\n",
    "        return best_loss\n",
    "\n",
    "\n",
    "def run_all(logdir, data, verbose=True):\n",
    "    best_loss = TF['best_loss']\n",
    "    rng = random.Random(0)\n",
    "\n",
    "    with tf.summary.create_file_writer(logdir).as_default():\n",
    "        hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "\n",
    "    num_experiments = 2\n",
    "    session_index = 0\n",
    "    for group_index in range(num_experiments):\n",
    "        hparams = {h: h.domain.sample_uniform(rng) for h in HPARAMS}\n",
    "        hparams_string = str(hparams)\n",
    "\n",
    "        session_id = str(session_index)\n",
    "        session_index += 1\n",
    "        print(\"--- Running experiment {}/{}\".format(session_index, num_experiments))\n",
    "\n",
    "        if verbose:\n",
    "            print(hparams_string)\n",
    "        \n",
    "        best_loss = run_single(\n",
    "            data=data,\n",
    "            base_logdir=logdir,\n",
    "            session_id=session_id,\n",
    "            hparams=hparams,\n",
    "            best_loss=best_loss,\n",
    "        )\n",
    "\n",
    "    # Update the best accuracy - temporary (dont choose models based on test performance)\n",
    "    config = ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    config_file = open(\"config.ini\", \"w\")\n",
    "    config.set(\"TF\", \"best_loss\", str(best_loss))\n",
    "    config.write(config_file)\n",
    "    config_file.close() \n",
    "\n",
    "np.random.seed(0)\n",
    "model_name = datetime.datetime.now().strftime(\"face %Y_%m_%d %H_%M_%S\")\n",
    "logdir = 'logs/{}'.format(model_name)\n",
    "run_all(logdir=logdir, data=data, verbose=True)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Running experiment 1/2\n",
      "{HParam(name='optimizer', domain=Discrete(['adagrad', 'adam', 'rmsprop']), display_name=None, description=None): 'adam', HParam(name='conv_kernel_size', domain=Discrete([3, 5, 7, 9]), display_name=None, description=None): 9, HParam(name='conv_layers', domain=IntInterval(1, 5), display_name=None, description=None): 1, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.38963946590857523}\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Relu (defined at <ipython-input-3-5847724ccfce>:85) ]]\n\t [[ReadVariableOp/_26]]\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Relu (defined at <ipython-input-3-5847724ccfce>:85) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1086]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5847724ccfce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"face %Y_%m_%d %H_%M_%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[0mlogdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'logs/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m \u001b[0mrun_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-5847724ccfce>\u001b[0m in \u001b[0;36mrun_all\u001b[1;34m(logdir, data, verbose)\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         best_loss = run_single(\n\u001b[0m\u001b[0;32m    126\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mbase_logdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-5847724ccfce>\u001b[0m in \u001b[0;36mrun_single\u001b[1;34m(data, base_logdir, session_id, hparams, best_loss)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mhparams_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     history = model.fit(\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'face'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'targets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Relu (defined at <ipython-input-3-5847724ccfce>:85) ]]\n\t [[ReadVariableOp/_26]]\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Relu (defined at <ipython-input-3-5847724ccfce>:85) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1086]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Multiple input, multiple data types"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Running experiment 1/2\n",
      "{HParam(name='optimizer', domain=Discrete(['adagrad', 'adam', 'rmsprop']), display_name=None, description=None): 'adam', HParam(name='conv_kernel_size', domain=Discrete([3, 5, 7, 9]), display_name=None, description=None): 9, HParam(name='conv_layers', domain=IntInterval(1, 5), display_name=None, description=None): 1, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.38963946590857523}\n",
      "Epoch 1/2\n",
      " 1/98 [..............................] - ETA: 0s - loss: 791059.6250 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.0314s). Check your callbacks.\n",
      "98/98 [==============================] - 5s 51ms/step - loss: 495909.7500 - accuracy: 0.7529 - val_loss: 155016.6562 - val_accuracy: 0.7529\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 5s 47ms/step - loss: 150835.9219 - accuracy: 0.7529 - val_loss: 145008.0781 - val_accuracy: 0.7529\n",
      "--- Running experiment 2/2\n",
      "{HParam(name='optimizer', domain=Discrete(['adagrad', 'adam', 'rmsprop']), display_name=None, description=None): 'adam', HParam(name='conv_kernel_size', domain=Discrete([3, 5, 7, 9]), display_name=None, description=None): 9, HParam(name='conv_layers', domain=IntInterval(1, 5), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.20741481240849652}\n",
      "Epoch 1/2\n",
      " 1/98 [..............................] - ETA: 0s - loss: 759391.4375 - accuracy: 0.8203WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.0319s). Check your callbacks.\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 332413.3125 - accuracy: 0.7540 - val_loss: 153580.5625 - val_accuracy: 0.7529\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 5s 49ms/step - loss: 145303.0938 - accuracy: 0.7529 - val_loss: 128636.8828 - val_accuracy: 0.7529\n"
     ]
    }
   ],
   "source": [
    "HP_OPTIMIZER = hp.HParam(\"optimizer\", hp.Discrete([\"adam\", \"rmsprop\", \"adagrad\"]))\n",
    "HP_CONV_KERNEL_SIZE = hp.HParam(\"conv_kernel_size\", hp.Discrete([3, 5, 7, 9]))\n",
    "HP_CONV_LAYERS = hp.HParam(\"conv_layers\", hp.IntInterval(1, 5))\n",
    "HP_DENSE_LAYERS = hp.HParam(\"dense_layers\", hp.IntInterval(1, 3))\n",
    "HP_DROPOUT = hp.HParam(\"dropout\", hp.RealInterval(0.1, 0.4))\n",
    "\n",
    "HPARAMS = [\n",
    "    HP_OPTIMIZER,\n",
    "    HP_CONV_KERNEL_SIZE,\n",
    "    HP_CONV_LAYERS,\n",
    "    HP_DENSE_LAYERS,\n",
    "    HP_DROPOUT,\n",
    "]\n",
    "\n",
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"epoch_loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"loss (val)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"batch_loss\",\n",
    "        group=\"train\",\n",
    "        display_name=\"loss (train)\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "def build_model(hparams, seed=87):\n",
    "    face_input = layers.Input(shape=(SETTINGS['image_size'], SETTINGS['image_size'], 3), name=\"face\")\n",
    "    l_eye_input = layers.Input(shape=(SETTINGS['image_size'], SETTINGS['image_size'], 3), name=\"l_eye\")\n",
    "    r_eye_input = layers.Input(shape=(SETTINGS['image_size'], SETTINGS['image_size'], 3), name=\"r_eye\")\n",
    "\n",
    "    conv_filters = 8\n",
    "    face = layers.Conv2D(filters=conv_filters, kernel_size=3, activation='relu')(face_input)\n",
    "    face = layers.MaxPool2D(pool_size=2, padding=\"same\")(face)\n",
    "\n",
    "    for _ in range(hparams[HP_CONV_LAYERS]):\n",
    "        face = layers.Conv2D(filters=conv_filters, kernel_size=3, activation='relu')(face)\n",
    "        face = layers.MaxPool2D(pool_size=2, padding=\"same\")(face)\n",
    "        conv_filters *= 2\n",
    "\n",
    "    face = layers.Flatten()(face)\n",
    "    face = layers.Dense(16, activation=\"relu\")(face)\n",
    "    #face_model = tf.keras.Model(inputs=face_input, outputs=face, name=\"face_model\")\n",
    "\n",
    "    l_eye = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(l_eye_input)\n",
    "    l_eye = layers.MaxPool2D(pool_size=2, padding=\"same\")(l_eye)\n",
    "    l_eye = layers.Flatten()(l_eye)\n",
    "    l_eye = layers.Dense(16, activation=\"relu\")(l_eye)\n",
    "    #l_eye_model = tf.keras.Model(inputs=l_eye_input, outputs=l_eye, name=\"l_eye_model\")\n",
    "\n",
    "    r_eye = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(r_eye_input)\n",
    "    r_eye = layers.MaxPool2D(pool_size=2, padding=\"same\")(r_eye)\n",
    "    r_eye = layers.Flatten()(r_eye)\n",
    "    r_eye = layers.Dense(16, activation=\"relu\")(r_eye)\n",
    "    #r_eye_model = tf.keras.Model(inputs=r_eye_input, outputs=r_eye, name=\"r_eye_model\")\n",
    "\n",
    "    combined = layers.concatenate([face, l_eye, r_eye])\n",
    "\n",
    "    dense_neurons = 16\n",
    "    output = layers.Dense(dense_neurons, activation=\"relu\")(combined)\n",
    "\n",
    "    for _ in range(hparams[HP_DENSE_LAYERS]):\n",
    "        output = layers.Dense(dense_neurons, activation=\"relu\")(output)\n",
    "        dense_neurons *= 2\n",
    "\n",
    "    output = layers.Dense(2)(output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[face_input, l_eye_input, r_eye_input], outputs=output)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_single(data, base_logdir, session_id, hparams):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = build_model(hparams=hparams, seed=session_id)\n",
    "\n",
    "    logdir = os.path.join(base_logdir, session_id)\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "        logdir,\n",
    "        histogram_freq=0,\n",
    "        update_freq=500,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    )\n",
    "    hparams_callback = hp.KerasCallback(logdir, hparams)\n",
    "\n",
    "    history = model.fit(\n",
    "            x=[data['face']['train'], data['l_eye']['train'], data['r_eye']['train']],\n",
    "            y=data['targets']['train'],\n",
    "            batch_size=128,\n",
    "            epochs=2,\n",
    "            validation_data=([data['face']['val'], data['l_eye']['val'], data['r_eye']['val']], data['targets']['val']),\n",
    "            callbacks=[tb_callback, hparams_callback],\n",
    "            verbose=1\n",
    "    )\n",
    "\n",
    "def run_all(logdir, data, verbose=True):\n",
    "    rng = random.Random(0)\n",
    "\n",
    "    with tf.summary.create_file_writer(logdir).as_default():\n",
    "        hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "\n",
    "    num_experiments = 2\n",
    "    session_index = 0\n",
    "    for group_index in range(num_experiments):\n",
    "        hparams = {h: h.domain.sample_uniform(rng) for h in HPARAMS}\n",
    "        hparams_string = str(hparams)\n",
    "\n",
    "        session_id = str(session_index)\n",
    "        session_index += 1\n",
    "        print(\"--- Running experiment {}/{}\".format(session_index, num_experiments))\n",
    "\n",
    "        if verbose:\n",
    "            print(hparams_string)\n",
    "        \n",
    "        run_single(\n",
    "            data=data,\n",
    "            base_logdir=logdir,\n",
    "            session_id=session_id,\n",
    "            hparams=hparams\n",
    "        )\n",
    "\n",
    "np.random.seed(0)\n",
    "model_name = datetime.datetime.now().strftime(\"full %Y_%m_%d %H_%M_%S\")\n",
    "logdir = 'logs/{}'.format(model_name)\n",
    "run_all(logdir=logdir, data=data, verbose=True)\n"
   ]
  }
 ]
}