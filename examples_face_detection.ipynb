{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import dlib\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dlib HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "hogFaceDetector = dlib.get_frontal_face_detector()\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faceRects = hogFaceDetector(frame, 0)\n",
    "    for faceRect in faceRects:\n",
    "        x1 = faceRect.left()\n",
    "        y1 = faceRect.top()\n",
    "        x2 = faceRect.right()\n",
    "        y2 = faceRect.bottom()\n",
    "\n",
    "        cv2.rectangle(frame, (x1,y1), (x2,y2), (0,0,255), 3)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "modelFile = \"trained_models/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"trained_models/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    height, width = frame.shape[0], frame.shape[1]\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.8:\n",
    "            x1 = int(detections[0, 0, i, 3] * width)\n",
    "            y1 = int(detections[0, 0, i, 4] * height)\n",
    "            x2 = int(detections[0, 0, i, 5] * width)\n",
    "            y2 = int(detections[0, 0, i, 6] * height)\n",
    "\n",
    "            cv2.rectangle(frame, (x1,y1), (x2,y2), (0,0,255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark detection (68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "\t# initialize the list of (x, y)-coordinates\n",
    "\tcoords = np.zeros((68, 2), dtype=dtype)\n",
    "\t# loop over the 68 facial landmarks and convert them\n",
    "\t# to a 2-tuple of (x, y)-coordinates\n",
    "\tfor i in range(0, 68):\n",
    "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\t# return the list of (x, y)-coordinates\n",
    "\treturn coords\n",
    "\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "\t(\"mouth\", (48, 68)),\n",
    "\t(\"right_eyebrow\", (17, 22)),\n",
    "\t(\"left_eyebrow\", (22, 27)),\n",
    "\t(\"right_eye\", (36, 42)),\n",
    "\t(\"left_eye\", (42, 48)),\n",
    "\t(\"nose\", (27, 35)),\n",
    "\t(\"jaw\", (0, 17))\n",
    "])\n",
    "\n",
    "SHOW_MARKERS = True\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"trained_models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(frame, 0)\n",
    "\n",
    "    if rects:\n",
    "        for (r, rect) in enumerate(rects):\n",
    "            x1 = rect.left()\n",
    "            y1 = rect.top()\n",
    "            x2 = rect.right()\n",
    "            y2 = rect.bottom()\n",
    "\n",
    "            if SHOW_MARKERS:\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,0,255), 2)\n",
    "            \n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            face = cv2.resize(face, (500,500), interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imshow(\"Face\", face)\n",
    "\n",
    "            # determine the facial landmarks for the face region, then\n",
    "            # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "            shape = predictor(frame, rect)\n",
    "            shape = shape_to_np(shape)\n",
    "\n",
    "            for (name, (i, j)) in FACIAL_LANDMARKS_IDXS.items():\n",
    "                if SHOW_MARKERS:\n",
    "                    # loop over the subset of facial landmarks, drawing the\n",
    "                    # specific face part\n",
    "                    for (x, y) in shape[i:j]:\n",
    "                        cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "                if name == \"right_eye\" or name == \"left_eye\":\n",
    "                    # extract the ROI of the face region as a separate image\n",
    "                    (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))\n",
    "                    roi = frame[y:y + h, x:x + w]\n",
    "                    roi = cv2.resize(roi, (500,250), interpolation=cv2.INTER_CUBIC)\n",
    "                    cv2.imshow(name, roi)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "\t# initialize the list of (x, y)-coordinates\n",
    "\tcoords = np.zeros((68, 2), dtype=dtype)\n",
    "\t# loop over the 68 facial landmarks and convert them\n",
    "\t# to a 2-tuple of (x, y)-coordinates\n",
    "\tfor i in range(0, 68):\n",
    "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\t# return the list of (x, y)-coordinates\n",
    "\treturn coords\n",
    "\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "\t(\"mouth\", (48, 68)),\n",
    "\t(\"right_eyebrow\", (17, 22)),\n",
    "\t(\"left_eyebrow\", (22, 27)),\n",
    "\t(\"right_eye\", (36, 42)),\n",
    "\t(\"left_eye\", (42, 48)),\n",
    "\t(\"nose\", (27, 35)),\n",
    "\t(\"jaw\", (0, 17))\n",
    "])\n",
    "\n",
    "SHOW_MARKERS = False\n",
    "\n",
    "modelFile = \"trained_models/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"trained_models/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "predictor = dlib.shape_predictor(\"trained_models/shape_predictor_68_face_landmarks.dat\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    height, width = frame.shape[0], frame.shape[1]\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    for d in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, d, 2]\n",
    "        if confidence > 0.9:\n",
    "            x1 = int(detections[0, 0, d, 3] * width)\n",
    "            y1 = int(detections[0, 0, d, 4] * height)\n",
    "            x2 = int(detections[0, 0, d, 5] * width)\n",
    "            y2 = int(detections[0, 0, d, 6] * height)\n",
    "\n",
    "            if SHOW_MARKERS:\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,0,255), 2)\n",
    "        \n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            face = cv2.resize(face, (500,500), interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imshow(\"Face\", face)\n",
    "\n",
    "            # determine the facial landmarks for the face region, then\n",
    "            # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "            rect = dlib.rectangle(x1,y1,x2,y2)\n",
    "            shape = predictor(frame, rect)\n",
    "            shape = shape_to_np(shape)\n",
    "\n",
    "            for (name, (i, j)) in FACIAL_LANDMARKS_IDXS.items():\n",
    "                if SHOW_MARKERS:\n",
    "                    # loop over the subset of facial landmarks, drawing the\n",
    "                    # specific face part\n",
    "                    for (x, y) in shape[i:j]:\n",
    "                        cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "                if name == \"right_eye\" or name == \"left_eye\":\n",
    "                    # extract the ROI of the face region as a separate image\n",
    "                    (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))\n",
    "                    roi = frame[y:y + h, x:x + w]\n",
    "                    roi = cv2.resize(roi, (500,250), interpolation=cv2.INTER_CUBIC)\n",
    "                    cv2.imshow(name, roi)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark detection (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "\t# initialize the list of (x, y)-coordinates\n",
    "\tcoords = np.zeros((5, 2), dtype=dtype)\n",
    "\t# loop over the 68 facial landmarks and convert them\n",
    "\t# to a 2-tuple of (x, y)-coordinates\n",
    "\tfor i in range(0, 5):\n",
    "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\t# return the list of (x, y)-coordinates\n",
    "\treturn coords\n",
    "\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "\t(\"right_eye\", (0, 2)),\n",
    "\t(\"left_eye\", (2, 4))\n",
    "])\n",
    "\n",
    "SHOW_MARKERS = True\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"trained_models/shape_predictor_5_face_landmarks.dat\")\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(frame, 0)\n",
    "\n",
    "    if rects:\n",
    "        for (r, rect) in enumerate(rects):\n",
    "            x1 = rect.left()\n",
    "            y1 = rect.top()\n",
    "            x2 = rect.right()\n",
    "            y2 = rect.bottom()\n",
    "\n",
    "            if SHOW_MARKERS:\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,0,255), 2)\n",
    "            \n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            face = cv2.resize(face, (500,500), interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imshow(\"Face\", face)\n",
    "\n",
    "            # determine the facial landmarks for the face region, then\n",
    "            # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "            shape = predictor(frame, rect)\n",
    "            shape = shape_to_np(shape)\n",
    "\n",
    "            for (name, (i, j)) in FACIAL_LANDMARKS_IDXS.items():\n",
    "                if SHOW_MARKERS:\n",
    "                    # loop over the subset of facial landmarks, drawing the\n",
    "                    # specific face part\n",
    "                    for (x, y) in shape[i:j]:\n",
    "                        cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "                if name == \"right_eye\" or name == \"left_eye\":\n",
    "                    # extract the ROI of the face region as a separate image\n",
    "                    (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))\n",
    "                    roi = frame[y-10:y + h+10, x:x + w]\n",
    "                    roi = cv2.resize(roi, (500,300), interpolation=cv2.INTER_CUBIC)\n",
    "                    cv2.imshow(name, roi)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "\t# initialize the list of (x, y)-coordinates\n",
    "\tcoords = np.zeros((5, 2), dtype=dtype)\n",
    "\t# loop over the 68 facial landmarks and convert them\n",
    "\t# to a 2-tuple of (x, y)-coordinates\n",
    "\tfor i in range(0, 5):\n",
    "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\t# return the list of (x, y)-coordinates\n",
    "\treturn coords\n",
    "\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "\t(\"right_eye\", (0, 2)),\n",
    "\t(\"left_eye\", (2, 4))\n",
    "])\n",
    "\n",
    "SHOW_MARKERS = True\n",
    "\n",
    "modelFile = \"trained_models/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"trained_models/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "predictor = dlib.shape_predictor(\"trained_models/shape_predictor_5_face_landmarks.dat\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    height, width = frame.shape[0], frame.shape[1]\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    for d in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, d, 2]\n",
    "        if confidence > 0.9:\n",
    "            x1 = int(detections[0, 0, d, 3] * width)\n",
    "            y1 = int(detections[0, 0, d, 4] * height)\n",
    "            x2 = int(detections[0, 0, d, 5] * width)\n",
    "            y2 = int(detections[0, 0, d, 6] * height)\n",
    "\n",
    "            if SHOW_MARKERS:\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,0,255), 2)\n",
    "        \n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            face = cv2.resize(face, (500,500), interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imshow(\"Face\", face)\n",
    "\n",
    "            # determine the facial landmarks for the face region, then\n",
    "            # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "            rect = dlib.rectangle(x1,y1,x2,y2)\n",
    "            shape = predictor(frame, rect)\n",
    "            shape = shape_to_np(shape)\n",
    "\n",
    "            for (name, (i, j)) in FACIAL_LANDMARKS_IDXS.items():\n",
    "                if SHOW_MARKERS:\n",
    "                    # loop over the subset of facial landmarks, drawing the\n",
    "                    # specific face part\n",
    "                    for (x, y) in shape[i:j]:\n",
    "                        cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "                if name == \"right_eye\" or name == \"left_eye\":\n",
    "                    # extract the ROI of the face region as a separate image\n",
    "                    (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))\n",
    "                    roi = frame[y-10:y + h+10, x:x + w]\n",
    "                    roi = cv2.resize(roi, (500,300), interpolation=cv2.INTER_CUBIC)\n",
    "                    cv2.imshow(name, roi)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "source": [
    "# Face alignment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## dlib"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "\t# initialize the list of (x, y)-coordinates\n",
    "\tcoords = np.zeros((5, 2), dtype=dtype)\n",
    "\t# loop over the facial landmarks and convert them\n",
    "\t# to a 2-tuple of (x, y)-coordinates\n",
    "\tfor i in range(0, 5):\n",
    "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\t# return the list of (x, y)-coordinates\n",
    "\treturn coords\n",
    "\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "\t(\"right_eye\", (0, 2)),\n",
    "\t(\"left_eye\", (2, 4)),\n",
    "    (\"nose\", (4, 5))\n",
    "])\n",
    "\n",
    "SHOW_MARKERS = True\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"trained_models/shape_predictor_5_face_landmarks.dat\")\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    rects = detector(frame, 0)\n",
    "\n",
    "    if len(rects) > 0:\n",
    "        faces = dlib.full_object_detections()\n",
    "        for detection in rects:\n",
    "            shape = predictor(frame, detection)\n",
    "            faces.append(shape)\n",
    "            \n",
    "            for (name, (i, j)) in FACIAL_LANDMARKS_IDXS.items():\n",
    "                if SHOW_MARKERS:\n",
    "                    # loop over the subset of facial landmarks, drawing the\n",
    "                    # specific face part\n",
    "                    for (x, y) in shape_to_np(shape)[i:j]:\n",
    "                        cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "        aligned = dlib.get_face_chips(frame, faces, size=160)\n",
    "        for image in aligned:\n",
    "            cv2.imshow(\"aligned\", image)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}