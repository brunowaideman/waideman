{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from utils import get_config\n",
    "\n",
    "# Read config.ini file\n",
    "SETTINGS, COLOURS, EYETRACKER, TF = get_config(\"config.ini\")\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, *img_types):\n",
    "        df = pd.read_csv(\"f:/eyetracker/data/positions.csv\")\n",
    "        df[\"filename\"] = df[\"id\"].astype(\"str\") + \".jpg\"\n",
    "\n",
    "        self.img_types = list(img_types)\n",
    "        self.filenames = df[\"filename\"].tolist()\n",
    "        self.targets = torch.Tensor(list(zip(df[\"x\"], df[\"y\"])))\n",
    "        self.head_angle = torch.Tensor(df['head_angle'].tolist())\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"targets\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "        if \"head_angle\" in self.img_types:\n",
    "            sample[\"head_angle\"] = self.head_angle[idx]\n",
    "\n",
    "        for img_type in self.img_types:\n",
    "            img = Image.open(\"f:/eyetracker/data/{}/{}\".format(img_type, self.filenames[idx]))\n",
    "            img = self.transform(img)\n",
    "            sample[img_type] = img\n",
    "\n",
    "        return sample\n",
    "\n",
    "def create_datasets(*img_types, batch_size=1, train_prop = 0.8, val_prop = 0.1, seed=87):\n",
    "    dataset = FaceDataset(*img_types)\n",
    "    n_train = int(len(dataset) * train_prop)\n",
    "    n_val = int(len(dataset) * val_prop)\n",
    "    n_test = len(dataset) - n_train - n_val\n",
    "    ds_train, ds_val, ds_test = random_split(dataset, (n_train, n_val, n_test), generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# d_train, d_val, d_test = create_datasets('l_eye', batch_size=1)\n",
    "# for b in d_train:\n",
    "#     print(b['targets'])\n",
    "#     print(b['l_eye'].shape)\n",
    "#     image = torch.squeeze(b['l_eye'][i]).permute(1, 2, 0)\n",
    "#     plt.imshow(image.numpy())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model experiments\n",
    "## Single face image pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 657231.812\n",
      "[1,    11] loss: 684998.307\n",
      "[1,    21] loss: 353023.360\n",
      "[1,    31] loss: 245015.244\n",
      "[1,    41] loss: 188852.800\n",
      "[1,    51] loss: 146138.311\n",
      "[1,    61] loss: 123694.043\n",
      "[1,    71] loss: 106071.651\n",
      "[1,    81] loss: 92342.431\n",
      "[1,    91] loss: 83465.986\n",
      "[2,     1] loss: 736727.188\n",
      "[2,    11] loss: 698145.756\n",
      "[2,    21] loss: 345437.571\n",
      "[2,    31] loss: 242512.704\n",
      "[2,    41] loss: 184162.627\n",
      "[2,    51] loss: 146629.626\n",
      "[2,    61] loss: 124668.499\n",
      "[2,    71] loss: 105205.978\n",
      "[2,    81] loss: 94193.040\n",
      "[2,    91] loss: 83074.088\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 31 * 31, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 31 * 31)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(d_train, d_val, d_test, device):\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(d_train, 0):\n",
    "            inputs, labels = data['face'].to(device), data['targets'].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 10 == 0:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        for i, data in enumerate(d_val, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data['face'].to(device), data['targets'].to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "\n",
    "def test(net, d_test, device):\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = d_test['face'].to(device), d_test['targets'].to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_steps = 0\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.cpu().numpy()\n",
    "        val_steps += 1\n",
    "\n",
    "    return val_loss / val_steps\n",
    "\n",
    "\n",
    "def run():\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "    d_train, d_val, d_test = create_datasets('face', batch_size=128)\n",
    "\n",
    "    train(d_train, d_val, d_test, device)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr:  85%|████████▌ | 85/100 [00:14<00:02,  5.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  86%|████████▌ | 86/100 [00:14<00:02,  6.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  87%|████████▋ | 87/100 [00:14<00:02,  6.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  88%|████████▊ | 88/100 [00:14<00:01,  6.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  89%|████████▉ | 89/100 [00:14<00:01,  6.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  90%|█████████ | 90/100 [00:15<00:01,  5.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  91%|█████████ | 91/100 [00:15<00:01,  5.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  92%|█████████▏| 92/100 [00:15<00:01,  5.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  93%|█████████▎| 93/100 [00:15<00:01,  5.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  94%|█████████▍| 94/100 [00:15<00:01,  5.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:15<00:00,  5.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  96%|█████████▌| 96/100 [00:16<00:00,  5.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:16<00:00,  6.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:16<00:00,  5.94it/s]\n",
      "\n",
      "  | Name  | Type      | Params | In sizes        | Out sizes      \n",
      "------------------------------------------------------------------------\n",
      "0 | conv1 | Conv2d    | 448    | [1, 3, 64, 64]  | [1, 16, 62, 62]\n",
      "1 | pool1 | MaxPool2d | 0      | [1, 16, 62, 62] | [1, 16, 31, 31]\n",
      "2 | conv2 | Conv2d    | 4.6 K  | [1, 16, 31, 31] | [1, 32, 29, 29]\n",
      "3 | pool2 | MaxPool2d | 0      | [1, 32, 29, 29] | [1, 32, 14, 14]\n",
      "4 | fc1   | Linear    | 200 K  | [1, 6272]       | [1, 32]        \n",
      "5 | fc2   | Linear    | 66     | [1, 32]         | [1, 2]         \n",
      "------------------------------------------------------------------------\n",
      "205 K     Trainable params\n",
      "0         Non-trainable params\n",
      "205 K     Total params\n",
      "LR: 0.00478630092322638\n",
      "Epoch 1:  88%|████████▊ | 98/111 [00:16<00:02,  5.91it/s, loss=1.33e+05, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  90%|█████████ | 100/111 [00:16<00:01,  5.98it/s, loss=1.33e+05, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.44it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.46it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.49it/s]\u001b[A\n",
      "Epoch 1:  94%|█████████▎| 104/111 [00:17<00:01,  5.99it/s, loss=1.33e+05, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.21it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.22it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.15it/s]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 108/111 [00:17<00:00,  5.99it/s, loss=1.33e+05, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.16it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.27it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 111/111 [00:18<00:00,  5.99it/s, loss=1.33e+05, v_num=25]\n",
      "Epoch 2:  88%|████████▊ | 98/111 [00:16<00:02,  5.91it/s, loss=8.65e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  90%|█████████ | 100/111 [00:16<00:01,  5.97it/s, loss=8.65e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.67it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.68it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.77it/s]\u001b[A\n",
      "Epoch 2:  94%|█████████▎| 104/111 [00:17<00:01,  5.99it/s, loss=8.65e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.45it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.48it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.51it/s]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 108/111 [00:17<00:00,  6.01it/s, loss=8.65e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.51it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.40it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 111/111 [00:18<00:00,  6.01it/s, loss=8.65e+04, v_num=25]\n",
      "Epoch 3:  88%|████████▊ | 98/111 [00:15<00:02,  6.13it/s, loss=7.77e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 100/111 [00:15<00:01,  6.19it/s, loss=7.77e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.11it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.25it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.38it/s]\u001b[A\n",
      "Epoch 3:  94%|█████████▎| 104/111 [00:16<00:01,  6.20it/s, loss=7.77e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.34it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.43it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.40it/s]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 108/111 [00:17<00:00,  6.21it/s, loss=7.77e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.46it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.61it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 111/111 [00:17<00:00,  6.21it/s, loss=7.77e+04, v_num=25]\n",
      "Epoch 4:  88%|████████▊ | 98/111 [00:15<00:02,  6.10it/s, loss=6.7e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4:  90%|█████████ | 100/111 [00:16<00:01,  6.16it/s, loss=6.7e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.10it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.15it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.15it/s]\u001b[A\n",
      "Epoch 4:  94%|█████████▎| 104/111 [00:16<00:01,  6.16it/s, loss=6.7e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.37it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.29it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.32it/s]\u001b[A\n",
      "Epoch 4:  97%|█████████▋| 108/111 [00:17<00:00,  6.16it/s, loss=6.7e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.12it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.22it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 111/111 [00:17<00:00,  6.16it/s, loss=6.7e+04, v_num=25]\n",
      "Epoch 5:  88%|████████▊ | 98/111 [00:16<00:02,  5.97it/s, loss=6.13e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5:  90%|█████████ | 100/111 [00:16<00:01,  6.03it/s, loss=6.13e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  5.94it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  5.60it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  5.75it/s]\u001b[A\n",
      "Epoch 5:  94%|█████████▎| 104/111 [00:17<00:01,  6.01it/s, loss=6.13e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:01<00:01,  5.81it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:01,  5.98it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.06it/s]\u001b[A\n",
      "Epoch 5:  97%|█████████▋| 108/111 [00:17<00:00,  6.02it/s, loss=6.13e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.02it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  5.92it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 111/111 [00:18<00:00,  5.99it/s, loss=6.13e+04, v_num=25]\n",
      "Epoch 6:  88%|████████▊ | 98/111 [00:16<00:02,  5.86it/s, loss=5.67e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6:  90%|█████████ | 100/111 [00:16<00:01,  5.91it/s, loss=5.67e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  5.79it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  5.87it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  5.93it/s]\u001b[A\n",
      "Epoch 6:  94%|█████████▎| 104/111 [00:17<00:01,  5.92it/s, loss=5.67e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.02it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.21it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  5.93it/s]\u001b[A\n",
      "Epoch 6:  97%|█████████▋| 108/111 [00:18<00:00,  5.92it/s, loss=5.67e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  5.96it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  5.99it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 111/111 [00:18<00:00,  5.91it/s, loss=5.67e+04, v_num=25]\n",
      "Epoch 7:  88%|████████▊ | 98/111 [00:16<00:02,  5.91it/s, loss=5.48e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7:  90%|█████████ | 100/111 [00:16<00:01,  5.97it/s, loss=5.48e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.01it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.01it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.09it/s]\u001b[A\n",
      "Epoch 7:  94%|█████████▎| 104/111 [00:17<00:01,  5.97it/s, loss=5.48e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.16it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.27it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.22it/s]\u001b[A\n",
      "Epoch 7:  97%|█████████▋| 108/111 [00:17<00:00,  5.98it/s, loss=5.48e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.15it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  5.83it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 111/111 [00:18<00:00,  5.95it/s, loss=5.48e+04, v_num=25]\n",
      "Epoch 8:  88%|████████▊ | 98/111 [00:16<00:02,  5.88it/s, loss=5.12e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8:  90%|█████████ | 100/111 [00:16<00:01,  5.94it/s, loss=5.12e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.23it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.34it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.37it/s]\u001b[A\n",
      "Epoch 8:  94%|█████████▎| 104/111 [00:17<00:01,  5.95it/s, loss=5.12e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.25it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.37it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.43it/s]\u001b[A\n",
      "Epoch 8:  97%|█████████▋| 108/111 [00:17<00:00,  5.97it/s, loss=5.12e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.54it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.52it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 111/111 [00:18<00:00,  5.97it/s, loss=5.12e+04, v_num=25]\n",
      "Epoch 9:  88%|████████▊ | 98/111 [00:16<00:02,  5.72it/s, loss=5.32e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9:  90%|█████████ | 100/111 [00:17<00:01,  5.78it/s, loss=5.32e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  5.98it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  5.85it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  5.71it/s]\u001b[A\n",
      "Epoch 9:  94%|█████████▎| 104/111 [00:17<00:01,  5.77it/s, loss=5.32e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:01<00:01,  5.49it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:01,  5.58it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  5.63it/s]\u001b[A\n",
      "Epoch 9:  97%|█████████▋| 108/111 [00:18<00:00,  5.76it/s, loss=5.32e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  5.26it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:02<00:00,  5.15it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 111/111 [00:19<00:00,  5.74it/s, loss=5.32e+04, v_num=25]\n",
      "Epoch 10:  88%|████████▊ | 98/111 [00:16<00:02,  5.96it/s, loss=4.81e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10:  90%|█████████ | 100/111 [00:16<00:01,  6.02it/s, loss=4.81e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.27it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.10it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.30it/s]\u001b[A\n",
      "Epoch 10:  94%|█████████▎| 104/111 [00:17<00:01,  6.03it/s, loss=4.81e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.36it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.43it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.40it/s]\u001b[A\n",
      "Epoch 10:  97%|█████████▋| 108/111 [00:17<00:00,  6.05it/s, loss=4.81e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.43it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.41it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 111/111 [00:18<00:00,  6.04it/s, loss=4.81e+04, v_num=25]\n",
      "Epoch 11:  88%|████████▊ | 98/111 [00:16<00:02,  5.92it/s, loss=4.6e+04, v_num=25] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11:  90%|█████████ | 100/111 [00:16<00:01,  5.99it/s, loss=4.6e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.54it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.53it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.45it/s]\u001b[A\n",
      "Epoch 11:  94%|█████████▎| 104/111 [00:17<00:01,  6.01it/s, loss=4.6e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.50it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.63it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.46it/s]\u001b[A\n",
      "Epoch 11:  97%|█████████▋| 108/111 [00:17<00:00,  6.02it/s, loss=4.6e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.36it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.28it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 111/111 [00:18<00:00,  6.01it/s, loss=4.6e+04, v_num=25]\n",
      "Epoch 12:  88%|████████▊ | 98/111 [00:15<00:02,  6.08it/s, loss=4.58e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12:  90%|█████████ | 100/111 [00:16<00:01,  6.13it/s, loss=4.58e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  5.64it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  5.84it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  5.95it/s]\u001b[A\n",
      "Epoch 12:  94%|█████████▎| 104/111 [00:16<00:01,  6.14it/s, loss=4.58e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.20it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.33it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.40it/s]\u001b[A\n",
      "Epoch 12:  97%|█████████▋| 108/111 [00:17<00:00,  6.16it/s, loss=4.58e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.22it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.32it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 111/111 [00:17<00:00,  6.15it/s, loss=4.58e+04, v_num=25]\n",
      "Epoch 13:  88%|████████▊ | 98/111 [00:16<00:02,  5.93it/s, loss=4.64e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13:  90%|█████████ | 100/111 [00:16<00:01,  5.99it/s, loss=4.64e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.13it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.12it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.21it/s]\u001b[A\n",
      "Epoch 13:  94%|█████████▎| 104/111 [00:17<00:01,  6.00it/s, loss=4.64e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.18it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.06it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.05it/s]\u001b[A\n",
      "Epoch 13:  97%|█████████▋| 108/111 [00:17<00:00,  6.00it/s, loss=4.64e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.18it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.14it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 111/111 [00:18<00:00,  6.00it/s, loss=4.64e+04, v_num=25]\n",
      "Epoch 14:  88%|████████▊ | 98/111 [00:16<00:02,  5.96it/s, loss=4.15e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14:  90%|█████████ | 100/111 [00:16<00:01,  6.02it/s, loss=4.15e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.34it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.45it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.44it/s]\u001b[A\n",
      "Epoch 14:  94%|█████████▎| 104/111 [00:17<00:01,  6.03it/s, loss=4.15e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.34it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.46it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  5.98it/s]\u001b[A\n",
      "Epoch 14:  97%|█████████▋| 108/111 [00:17<00:00,  6.03it/s, loss=4.15e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  5.73it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  5.92it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 111/111 [00:18<00:00,  6.02it/s, loss=4.15e+04, v_num=25]\n",
      "Epoch 15:  88%|████████▊ | 98/111 [00:16<00:02,  5.74it/s, loss=4.51e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15:  90%|█████████ | 100/111 [00:17<00:01,  5.80it/s, loss=4.51e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.68it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.54it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.44it/s]\u001b[A\n",
      "Epoch 15:  94%|█████████▎| 104/111 [00:17<00:01,  5.82it/s, loss=4.51e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.30it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.22it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.31it/s]\u001b[A\n",
      "Epoch 15:  97%|█████████▋| 108/111 [00:18<00:00,  5.84it/s, loss=4.51e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.35it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.38it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 111/111 [00:18<00:00,  5.84it/s, loss=4.51e+04, v_num=25]\n",
      "Epoch 16:  88%|████████▊ | 98/111 [00:16<00:02,  6.03it/s, loss=4.17e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16:  90%|█████████ | 100/111 [00:16<00:01,  6.09it/s, loss=4.17e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.46it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.49it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.47it/s]\u001b[A\n",
      "Epoch 16:  94%|█████████▎| 104/111 [00:16<00:01,  6.10it/s, loss=4.17e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.46it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.60it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.67it/s]\u001b[A\n",
      "Epoch 16:  97%|█████████▋| 108/111 [00:17<00:00,  6.12it/s, loss=4.17e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.49it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.45it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 111/111 [00:17<00:00,  6.12it/s, loss=4.17e+04, v_num=25]\n",
      "Epoch 17:  88%|████████▊ | 98/111 [00:15<00:02,  6.12it/s, loss=3.81e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17:  90%|█████████ | 100/111 [00:16<00:01,  6.18it/s, loss=3.81e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.20it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.20it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.26it/s]\u001b[A\n",
      "Epoch 17:  94%|█████████▎| 104/111 [00:16<00:01,  6.18it/s, loss=3.81e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.05it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.12it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.29it/s]\u001b[A\n",
      "Epoch 17:  97%|█████████▋| 108/111 [00:17<00:00,  6.18it/s, loss=3.81e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.30it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.21it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 111/111 [00:17<00:00,  6.17it/s, loss=3.81e+04, v_num=25]\n",
      "Epoch 18:  88%|████████▊ | 98/111 [00:15<00:02,  6.10it/s, loss=3.36e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18:  90%|█████████ | 100/111 [00:16<00:01,  6.17it/s, loss=3.36e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  6.46it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  6.52it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  6.53it/s]\u001b[A\n",
      "Epoch 18:  94%|█████████▎| 104/111 [00:16<00:01,  6.18it/s, loss=3.36e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:00<00:01,  6.50it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:00,  6.51it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  6.42it/s]\u001b[A\n",
      "Epoch 18:  97%|█████████▋| 108/111 [00:17<00:00,  6.19it/s, loss=3.36e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  6.32it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  6.40it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 111/111 [00:17<00:00,  6.18it/s, loss=3.36e+04, v_num=25]\n",
      "Epoch 19:  88%|████████▊ | 98/111 [00:15<00:02,  6.08it/s, loss=3.29e+04, v_num=25]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19:  90%|█████████ | 100/111 [00:16<00:01,  6.14it/s, loss=3.29e+04, v_num=25]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:01,  5.89it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:01,  5.96it/s]\u001b[A\n",
      "Validating:  31%|███       | 4/13 [00:00<00:01,  5.98it/s]\u001b[A\n",
      "Epoch 19:  94%|█████████▎| 104/111 [00:16<00:01,  6.13it/s, loss=3.29e+04, v_num=25]\n",
      "Validating:  46%|████▌     | 6/13 [00:01<00:01,  5.89it/s]\u001b[A\n",
      "Validating:  54%|█████▍    | 7/13 [00:01<00:01,  5.95it/s]\u001b[A\n",
      "Validating:  62%|██████▏   | 8/13 [00:01<00:00,  5.65it/s]\u001b[A\n",
      "Epoch 19:  97%|█████████▋| 108/111 [00:17<00:00,  6.12it/s, loss=3.29e+04, v_num=25]\n",
      "Validating:  77%|███████▋  | 10/13 [00:01<00:00,  5.67it/s]\u001b[A\n",
      "Validating:  85%|████████▍ | 11/13 [00:01<00:00,  5.76it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 111/111 [00:18<00:00,  6.09it/s, loss=3.29e+04, v_num=25]\n",
      "                                                           \u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 408.053125 262.19625\" width=\"408.053125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <metadata>\r\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n",
       "   <cc:Work>\r\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n",
       "    <dc:date>2020-12-26T10:14:35.548104</dc:date>\r\n",
       "    <dc:format>image/svg+xml</dc:format>\r\n",
       "    <dc:creator>\r\n",
       "     <cc:Agent>\r\n",
       "      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\r\n",
       "     </cc:Agent>\r\n",
       "    </dc:creator>\r\n",
       "   </cc:Work>\r\n",
       "  </rdf:RDF>\r\n",
       " </metadata>\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 262.19625 \r\n",
       "L 408.053125 262.19625 \r\n",
       "L 408.053125 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 66.053125 224.64 \r\n",
       "L 400.853125 224.64 \r\n",
       "L 400.853125 7.2 \r\n",
       "L 66.053125 7.2 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\">\r\n",
       "    <g id=\"xtick_1\">\r\n",
       "     <g id=\"line2d_1\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 0 3.5 \r\n",
       "\" id=\"m05c2bd1794\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.316761\" xlink:href=\"#m05c2bd1794\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_1\">\r\n",
       "      <!-- $\\mathdefault{10^{-7}}$ -->\r\n",
       "      <g transform=\"translate(107.566761 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 12.40625 8.296875 \r\n",
       "L 28.515625 8.296875 \r\n",
       "L 28.515625 63.921875 \r\n",
       "L 10.984375 60.40625 \r\n",
       "L 10.984375 69.390625 \r\n",
       "L 28.421875 72.90625 \r\n",
       "L 38.28125 72.90625 \r\n",
       "L 38.28125 8.296875 \r\n",
       "L 54.390625 8.296875 \r\n",
       "L 54.390625 0 \r\n",
       "L 12.40625 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-49\"/>\r\n",
       "        <path d=\"M 31.78125 66.40625 \r\n",
       "Q 24.171875 66.40625 20.328125 58.90625 \r\n",
       "Q 16.5 51.421875 16.5 36.375 \r\n",
       "Q 16.5 21.390625 20.328125 13.890625 \r\n",
       "Q 24.171875 6.390625 31.78125 6.390625 \r\n",
       "Q 39.453125 6.390625 43.28125 13.890625 \r\n",
       "Q 47.125 21.390625 47.125 36.375 \r\n",
       "Q 47.125 51.421875 43.28125 58.90625 \r\n",
       "Q 39.453125 66.40625 31.78125 66.40625 \r\n",
       "z\r\n",
       "M 31.78125 74.21875 \r\n",
       "Q 44.046875 74.21875 50.515625 64.515625 \r\n",
       "Q 56.984375 54.828125 56.984375 36.375 \r\n",
       "Q 56.984375 17.96875 50.515625 8.265625 \r\n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \r\n",
       "Q 6.59375 17.96875 6.59375 36.375 \r\n",
       "Q 6.59375 54.828125 13.0625 64.515625 \r\n",
       "Q 19.53125 74.21875 31.78125 74.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-48\"/>\r\n",
       "        <path d=\"M 10.59375 35.5 \r\n",
       "L 73.1875 35.5 \r\n",
       "L 73.1875 27.203125 \r\n",
       "L 10.59375 27.203125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-8722\"/>\r\n",
       "        <path d=\"M 8.203125 72.90625 \r\n",
       "L 55.078125 72.90625 \r\n",
       "L 55.078125 68.703125 \r\n",
       "L 28.609375 0 \r\n",
       "L 18.3125 0 \r\n",
       "L 43.21875 64.59375 \r\n",
       "L 8.203125 64.59375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-55\"/>\r\n",
       "       </defs>\r\n",
       "       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-55\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_2\">\r\n",
       "     <g id=\"line2d_2\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"195.40767\" xlink:href=\"#m05c2bd1794\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_2\">\r\n",
       "      <!-- $\\mathdefault{10^{-5}}$ -->\r\n",
       "      <g transform=\"translate(183.65767 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 10.796875 72.90625 \r\n",
       "L 49.515625 72.90625 \r\n",
       "L 49.515625 64.59375 \r\n",
       "L 19.828125 64.59375 \r\n",
       "L 19.828125 46.734375 \r\n",
       "Q 21.96875 47.46875 24.109375 47.828125 \r\n",
       "Q 26.265625 48.1875 28.421875 48.1875 \r\n",
       "Q 40.625 48.1875 47.75 41.5 \r\n",
       "Q 54.890625 34.8125 54.890625 23.390625 \r\n",
       "Q 54.890625 11.625 47.5625 5.09375 \r\n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \r\n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \r\n",
       "Q 12.796875 0.140625 7.71875 1.703125 \r\n",
       "L 7.71875 11.625 \r\n",
       "Q 12.109375 9.234375 16.796875 8.0625 \r\n",
       "Q 21.484375 6.890625 26.703125 6.890625 \r\n",
       "Q 35.15625 6.890625 40.078125 11.328125 \r\n",
       "Q 45.015625 15.765625 45.015625 23.390625 \r\n",
       "Q 45.015625 31 40.078125 35.4375 \r\n",
       "Q 35.15625 39.890625 26.703125 39.890625 \r\n",
       "Q 22.75 39.890625 18.8125 39.015625 \r\n",
       "Q 14.890625 38.140625 10.796875 36.28125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-53\"/>\r\n",
       "       </defs>\r\n",
       "       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_3\">\r\n",
       "     <g id=\"line2d_3\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"271.49858\" xlink:href=\"#m05c2bd1794\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_3\">\r\n",
       "      <!-- $\\mathdefault{10^{-3}}$ -->\r\n",
       "      <g transform=\"translate(259.74858 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 40.578125 39.3125 \r\n",
       "Q 47.65625 37.796875 51.625 33 \r\n",
       "Q 55.609375 28.21875 55.609375 21.1875 \r\n",
       "Q 55.609375 10.40625 48.1875 4.484375 \r\n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \r\n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \r\n",
       "Q 12.796875 0.390625 7.625 2.203125 \r\n",
       "L 7.625 11.71875 \r\n",
       "Q 11.71875 9.328125 16.59375 8.109375 \r\n",
       "Q 21.484375 6.890625 26.8125 6.890625 \r\n",
       "Q 36.078125 6.890625 40.9375 10.546875 \r\n",
       "Q 45.796875 14.203125 45.796875 21.1875 \r\n",
       "Q 45.796875 27.640625 41.28125 31.265625 \r\n",
       "Q 36.765625 34.90625 28.71875 34.90625 \r\n",
       "L 20.21875 34.90625 \r\n",
       "L 20.21875 43.015625 \r\n",
       "L 29.109375 43.015625 \r\n",
       "Q 36.375 43.015625 40.234375 45.921875 \r\n",
       "Q 44.09375 48.828125 44.09375 54.296875 \r\n",
       "Q 44.09375 59.90625 40.109375 62.90625 \r\n",
       "Q 36.140625 65.921875 28.71875 65.921875 \r\n",
       "Q 24.65625 65.921875 20.015625 65.03125 \r\n",
       "Q 15.375 64.15625 9.8125 62.3125 \r\n",
       "L 9.8125 71.09375 \r\n",
       "Q 15.4375 72.65625 20.34375 73.4375 \r\n",
       "Q 25.25 74.21875 29.59375 74.21875 \r\n",
       "Q 40.828125 74.21875 47.359375 69.109375 \r\n",
       "Q 53.90625 64.015625 53.90625 55.328125 \r\n",
       "Q 53.90625 49.265625 50.4375 45.09375 \r\n",
       "Q 46.96875 40.921875 40.578125 39.3125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-51\"/>\r\n",
       "       </defs>\r\n",
       "       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_4\">\r\n",
       "     <g id=\"line2d_4\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"347.589489\" xlink:href=\"#m05c2bd1794\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_4\">\r\n",
       "      <!-- $\\mathdefault{10^{-1}}$ -->\r\n",
       "      <g transform=\"translate(335.839489 239.238438)scale(0.1 -0.1)\">\r\n",
       "       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"text_5\">\r\n",
       "     <!-- Learning rate -->\r\n",
       "     <g transform=\"translate(200 252.916563)scale(0.1 -0.1)\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 9.8125 72.90625 \r\n",
       "L 19.671875 72.90625 \r\n",
       "L 19.671875 8.296875 \r\n",
       "L 55.171875 8.296875 \r\n",
       "L 55.171875 0 \r\n",
       "L 9.8125 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-76\"/>\r\n",
       "       <path d=\"M 56.203125 29.59375 \r\n",
       "L 56.203125 25.203125 \r\n",
       "L 14.890625 25.203125 \r\n",
       "Q 15.484375 15.921875 20.484375 11.0625 \r\n",
       "Q 25.484375 6.203125 34.421875 6.203125 \r\n",
       "Q 39.59375 6.203125 44.453125 7.46875 \r\n",
       "Q 49.3125 8.734375 54.109375 11.28125 \r\n",
       "L 54.109375 2.78125 \r\n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \r\n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \r\n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \r\n",
       "Q 5.515625 13.8125 5.515625 26.8125 \r\n",
       "Q 5.515625 40.234375 12.765625 48.109375 \r\n",
       "Q 20.015625 56 32.328125 56 \r\n",
       "Q 43.359375 56 49.78125 48.890625 \r\n",
       "Q 56.203125 41.796875 56.203125 29.59375 \r\n",
       "z\r\n",
       "M 47.21875 32.234375 \r\n",
       "Q 47.125 39.59375 43.09375 43.984375 \r\n",
       "Q 39.0625 48.390625 32.421875 48.390625 \r\n",
       "Q 24.90625 48.390625 20.390625 44.140625 \r\n",
       "Q 15.875 39.890625 15.1875 32.171875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-101\"/>\r\n",
       "       <path d=\"M 34.28125 27.484375 \r\n",
       "Q 23.390625 27.484375 19.1875 25 \r\n",
       "Q 14.984375 22.515625 14.984375 16.5 \r\n",
       "Q 14.984375 11.71875 18.140625 8.90625 \r\n",
       "Q 21.296875 6.109375 26.703125 6.109375 \r\n",
       "Q 34.1875 6.109375 38.703125 11.40625 \r\n",
       "Q 43.21875 16.703125 43.21875 25.484375 \r\n",
       "L 43.21875 27.484375 \r\n",
       "z\r\n",
       "M 52.203125 31.203125 \r\n",
       "L 52.203125 0 \r\n",
       "L 43.21875 0 \r\n",
       "L 43.21875 8.296875 \r\n",
       "Q 40.140625 3.328125 35.546875 0.953125 \r\n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \r\n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \r\n",
       "Q 6 8.015625 6 15.921875 \r\n",
       "Q 6 25.140625 12.171875 29.828125 \r\n",
       "Q 18.359375 34.515625 30.609375 34.515625 \r\n",
       "L 43.21875 34.515625 \r\n",
       "L 43.21875 35.40625 \r\n",
       "Q 43.21875 41.609375 39.140625 45 \r\n",
       "Q 35.0625 48.390625 27.6875 48.390625 \r\n",
       "Q 23 48.390625 18.546875 47.265625 \r\n",
       "Q 14.109375 46.140625 10.015625 43.890625 \r\n",
       "L 10.015625 52.203125 \r\n",
       "Q 14.9375 54.109375 19.578125 55.046875 \r\n",
       "Q 24.21875 56 28.609375 56 \r\n",
       "Q 40.484375 56 46.34375 49.84375 \r\n",
       "Q 52.203125 43.703125 52.203125 31.203125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-97\"/>\r\n",
       "       <path d=\"M 41.109375 46.296875 \r\n",
       "Q 39.59375 47.171875 37.8125 47.578125 \r\n",
       "Q 36.03125 48 33.890625 48 \r\n",
       "Q 26.265625 48 22.1875 43.046875 \r\n",
       "Q 18.109375 38.09375 18.109375 28.8125 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 20.953125 51.171875 25.484375 53.578125 \r\n",
       "Q 30.03125 56 36.53125 56 \r\n",
       "Q 37.453125 56 38.578125 55.875 \r\n",
       "Q 39.703125 55.765625 41.0625 55.515625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-114\"/>\r\n",
       "       <path d=\"M 54.890625 33.015625 \r\n",
       "L 54.890625 0 \r\n",
       "L 45.90625 0 \r\n",
       "L 45.90625 32.71875 \r\n",
       "Q 45.90625 40.484375 42.875 44.328125 \r\n",
       "Q 39.84375 48.1875 33.796875 48.1875 \r\n",
       "Q 26.515625 48.1875 22.3125 43.546875 \r\n",
       "Q 18.109375 38.921875 18.109375 30.90625 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 21.34375 51.125 25.703125 53.5625 \r\n",
       "Q 30.078125 56 35.796875 56 \r\n",
       "Q 45.21875 56 50.046875 50.171875 \r\n",
       "Q 54.890625 44.34375 54.890625 33.015625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-110\"/>\r\n",
       "       <path d=\"M 9.421875 54.6875 \r\n",
       "L 18.40625 54.6875 \r\n",
       "L 18.40625 0 \r\n",
       "L 9.421875 0 \r\n",
       "z\r\n",
       "M 9.421875 75.984375 \r\n",
       "L 18.40625 75.984375 \r\n",
       "L 18.40625 64.59375 \r\n",
       "L 9.421875 64.59375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-105\"/>\r\n",
       "       <path d=\"M 45.40625 27.984375 \r\n",
       "Q 45.40625 37.75 41.375 43.109375 \r\n",
       "Q 37.359375 48.484375 30.078125 48.484375 \r\n",
       "Q 22.859375 48.484375 18.828125 43.109375 \r\n",
       "Q 14.796875 37.75 14.796875 27.984375 \r\n",
       "Q 14.796875 18.265625 18.828125 12.890625 \r\n",
       "Q 22.859375 7.515625 30.078125 7.515625 \r\n",
       "Q 37.359375 7.515625 41.375 12.890625 \r\n",
       "Q 45.40625 18.265625 45.40625 27.984375 \r\n",
       "z\r\n",
       "M 54.390625 6.78125 \r\n",
       "Q 54.390625 -7.171875 48.1875 -13.984375 \r\n",
       "Q 42 -20.796875 29.203125 -20.796875 \r\n",
       "Q 24.46875 -20.796875 20.265625 -20.09375 \r\n",
       "Q 16.0625 -19.390625 12.109375 -17.921875 \r\n",
       "L 12.109375 -9.1875 \r\n",
       "Q 16.0625 -11.328125 19.921875 -12.34375 \r\n",
       "Q 23.78125 -13.375 27.78125 -13.375 \r\n",
       "Q 36.625 -13.375 41.015625 -8.765625 \r\n",
       "Q 45.40625 -4.15625 45.40625 5.171875 \r\n",
       "L 45.40625 9.625 \r\n",
       "Q 42.625 4.78125 38.28125 2.390625 \r\n",
       "Q 33.9375 0 27.875 0 \r\n",
       "Q 17.828125 0 11.671875 7.65625 \r\n",
       "Q 5.515625 15.328125 5.515625 27.984375 \r\n",
       "Q 5.515625 40.671875 11.671875 48.328125 \r\n",
       "Q 17.828125 56 27.875 56 \r\n",
       "Q 33.9375 56 38.28125 53.609375 \r\n",
       "Q 42.625 51.21875 45.40625 46.390625 \r\n",
       "L 45.40625 54.6875 \r\n",
       "L 54.390625 54.6875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-103\"/>\r\n",
       "       <path id=\"DejaVuSans-32\"/>\r\n",
       "       <path d=\"M 18.3125 70.21875 \r\n",
       "L 18.3125 54.6875 \r\n",
       "L 36.8125 54.6875 \r\n",
       "L 36.8125 47.703125 \r\n",
       "L 18.3125 47.703125 \r\n",
       "L 18.3125 18.015625 \r\n",
       "Q 18.3125 11.328125 20.140625 9.421875 \r\n",
       "Q 21.96875 7.515625 27.59375 7.515625 \r\n",
       "L 36.8125 7.515625 \r\n",
       "L 36.8125 0 \r\n",
       "L 27.59375 0 \r\n",
       "Q 17.1875 0 13.234375 3.875 \r\n",
       "Q 9.28125 7.765625 9.28125 18.015625 \r\n",
       "L 9.28125 47.703125 \r\n",
       "L 2.6875 47.703125 \r\n",
       "L 2.6875 54.6875 \r\n",
       "L 9.28125 54.6875 \r\n",
       "L 9.28125 70.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-116\"/>\r\n",
       "      </defs>\r\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\r\n",
       "      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "      <use x=\"115.486328\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "      <use x=\"176.765625\" xlink:href=\"#DejaVuSans-114\"/>\r\n",
       "      <use x=\"216.128906\" xlink:href=\"#DejaVuSans-110\"/>\r\n",
       "      <use x=\"279.507812\" xlink:href=\"#DejaVuSans-105\"/>\r\n",
       "      <use x=\"307.291016\" xlink:href=\"#DejaVuSans-110\"/>\r\n",
       "      <use x=\"370.669922\" xlink:href=\"#DejaVuSans-103\"/>\r\n",
       "      <use x=\"434.146484\" xlink:href=\"#DejaVuSans-32\"/>\r\n",
       "      <use x=\"465.933594\" xlink:href=\"#DejaVuSans-114\"/>\r\n",
       "      <use x=\"507.046875\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "      <use x=\"568.326172\" xlink:href=\"#DejaVuSans-116\"/>\r\n",
       "      <use x=\"607.535156\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_2\">\r\n",
       "    <g id=\"ytick_1\">\r\n",
       "     <g id=\"line2d_5\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L -3.5 0 \r\n",
       "\" id=\"m5bfa919857\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.053125\" xlink:href=\"#m5bfa919857\" y=\"210.203178\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_6\">\r\n",
       "      <!-- 575000 -->\r\n",
       "      <g transform=\"translate(20.878125 214.002397)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_2\">\r\n",
       "     <g id=\"line2d_6\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.053125\" xlink:href=\"#m5bfa919857\" y=\"183.623503\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_7\">\r\n",
       "      <!-- 600000 -->\r\n",
       "      <g transform=\"translate(20.878125 187.422721)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 33.015625 40.375 \r\n",
       "Q 26.375 40.375 22.484375 35.828125 \r\n",
       "Q 18.609375 31.296875 18.609375 23.390625 \r\n",
       "Q 18.609375 15.53125 22.484375 10.953125 \r\n",
       "Q 26.375 6.390625 33.015625 6.390625 \r\n",
       "Q 39.65625 6.390625 43.53125 10.953125 \r\n",
       "Q 47.40625 15.53125 47.40625 23.390625 \r\n",
       "Q 47.40625 31.296875 43.53125 35.828125 \r\n",
       "Q 39.65625 40.375 33.015625 40.375 \r\n",
       "z\r\n",
       "M 52.59375 71.296875 \r\n",
       "L 52.59375 62.3125 \r\n",
       "Q 48.875 64.0625 45.09375 64.984375 \r\n",
       "Q 41.3125 65.921875 37.59375 65.921875 \r\n",
       "Q 27.828125 65.921875 22.671875 59.328125 \r\n",
       "Q 17.53125 52.734375 16.796875 39.40625 \r\n",
       "Q 19.671875 43.65625 24.015625 45.921875 \r\n",
       "Q 28.375 48.1875 33.59375 48.1875 \r\n",
       "Q 44.578125 48.1875 50.953125 41.515625 \r\n",
       "Q 57.328125 34.859375 57.328125 23.390625 \r\n",
       "Q 57.328125 12.15625 50.6875 5.359375 \r\n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \r\n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \r\n",
       "Q 6.984375 17.96875 6.984375 36.375 \r\n",
       "Q 6.984375 53.65625 15.1875 63.9375 \r\n",
       "Q 23.390625 74.21875 37.203125 74.21875 \r\n",
       "Q 40.921875 74.21875 44.703125 73.484375 \r\n",
       "Q 48.484375 72.75 52.59375 71.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-54\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_3\">\r\n",
       "     <g id=\"line2d_7\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.053125\" xlink:href=\"#m5bfa919857\" y=\"157.043827\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_8\">\r\n",
       "      <!-- 625000 -->\r\n",
       "      <g transform=\"translate(20.878125 160.843046)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 19.1875 8.296875 \r\n",
       "L 53.609375 8.296875 \r\n",
       "L 53.609375 0 \r\n",
       "L 7.328125 0 \r\n",
       "L 7.328125 8.296875 \r\n",
       "Q 12.9375 14.109375 22.625 23.890625 \r\n",
       "Q 32.328125 33.6875 34.8125 36.53125 \r\n",
       "Q 39.546875 41.84375 41.421875 45.53125 \r\n",
       "Q 43.3125 49.21875 43.3125 52.78125 \r\n",
       "Q 43.3125 58.59375 39.234375 62.25 \r\n",
       "Q 35.15625 65.921875 28.609375 65.921875 \r\n",
       "Q 23.96875 65.921875 18.8125 64.3125 \r\n",
       "Q 13.671875 62.703125 7.8125 59.421875 \r\n",
       "L 7.8125 69.390625 \r\n",
       "Q 13.765625 71.78125 18.9375 73 \r\n",
       "Q 24.125 74.21875 28.421875 74.21875 \r\n",
       "Q 39.75 74.21875 46.484375 68.546875 \r\n",
       "Q 53.21875 62.890625 53.21875 53.421875 \r\n",
       "Q 53.21875 48.921875 51.53125 44.890625 \r\n",
       "Q 49.859375 40.875 45.40625 35.40625 \r\n",
       "Q 44.1875 33.984375 37.640625 27.21875 \r\n",
       "Q 31.109375 20.453125 19.1875 8.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-50\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_4\">\r\n",
       "     <g id=\"line2d_8\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.053125\" xlink:href=\"#m5bfa919857\" y=\"130.464151\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_9\">\r\n",
       "      <!-- 650000 -->\r\n",
       "      <g transform=\"translate(20.878125 134.26337)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_5\">\r\n",
       "     <g id=\"line2d_9\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.053125\" xlink:href=\"#m5bfa919857\" y=\"103.884475\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_10\">\r\n",
       "      <!-- 675000 -->\r\n",
       "      <g transform=\"translate(20.878125 107.683694)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_6\">\r\n",
       "     <g id=\"line2d_10\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.053125\" xlink:href=\"#m5bfa919857\" y=\"77.3048\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_11\">\r\n",
       "      <!-- 700000 -->\r\n",
       "      <g transform=\"translate(20.878125 81.104018)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-55\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_7\">\r\n",
       "     <g id=\"line2d_11\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.053125\" xlink:href=\"#m5bfa919857\" y=\"50.725124\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_12\">\r\n",
       "      <!-- 725000 -->\r\n",
       "      <g transform=\"translate(20.878125 54.524343)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-55\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_8\">\r\n",
       "     <g id=\"line2d_12\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.053125\" xlink:href=\"#m5bfa919857\" y=\"24.145448\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_13\">\r\n",
       "      <!-- 750000 -->\r\n",
       "      <g transform=\"translate(20.878125 27.944667)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-55\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"text_14\">\r\n",
       "     <!-- Loss -->\r\n",
       "     <g transform=\"translate(14.798438 126.887188)rotate(-90)scale(0.1 -0.1)\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 30.609375 48.390625 \r\n",
       "Q 23.390625 48.390625 19.1875 42.75 \r\n",
       "Q 14.984375 37.109375 14.984375 27.296875 \r\n",
       "Q 14.984375 17.484375 19.15625 11.84375 \r\n",
       "Q 23.34375 6.203125 30.609375 6.203125 \r\n",
       "Q 37.796875 6.203125 41.984375 11.859375 \r\n",
       "Q 46.1875 17.53125 46.1875 27.296875 \r\n",
       "Q 46.1875 37.015625 41.984375 42.703125 \r\n",
       "Q 37.796875 48.390625 30.609375 48.390625 \r\n",
       "z\r\n",
       "M 30.609375 56 \r\n",
       "Q 42.328125 56 49.015625 48.375 \r\n",
       "Q 55.71875 40.765625 55.71875 27.296875 \r\n",
       "Q 55.71875 13.875 49.015625 6.21875 \r\n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \r\n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \r\n",
       "Q 5.515625 13.875 5.515625 27.296875 \r\n",
       "Q 5.515625 40.765625 12.171875 48.375 \r\n",
       "Q 18.84375 56 30.609375 56 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-111\"/>\r\n",
       "       <path d=\"M 44.28125 53.078125 \r\n",
       "L 44.28125 44.578125 \r\n",
       "Q 40.484375 46.53125 36.375 47.5 \r\n",
       "Q 32.28125 48.484375 27.875 48.484375 \r\n",
       "Q 21.1875 48.484375 17.84375 46.4375 \r\n",
       "Q 14.5 44.390625 14.5 40.28125 \r\n",
       "Q 14.5 37.15625 16.890625 35.375 \r\n",
       "Q 19.28125 33.59375 26.515625 31.984375 \r\n",
       "L 29.59375 31.296875 \r\n",
       "Q 39.15625 29.25 43.1875 25.515625 \r\n",
       "Q 47.21875 21.78125 47.21875 15.09375 \r\n",
       "Q 47.21875 7.46875 41.1875 3.015625 \r\n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \r\n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \r\n",
       "Q 10.6875 0.296875 5.421875 2 \r\n",
       "L 5.421875 11.28125 \r\n",
       "Q 10.40625 8.6875 15.234375 7.390625 \r\n",
       "Q 20.0625 6.109375 24.8125 6.109375 \r\n",
       "Q 31.15625 6.109375 34.5625 8.28125 \r\n",
       "Q 37.984375 10.453125 37.984375 14.40625 \r\n",
       "Q 37.984375 18.0625 35.515625 20.015625 \r\n",
       "Q 33.0625 21.96875 24.703125 23.78125 \r\n",
       "L 21.578125 24.515625 \r\n",
       "Q 13.234375 26.265625 9.515625 29.90625 \r\n",
       "Q 5.8125 33.546875 5.8125 39.890625 \r\n",
       "Q 5.8125 47.609375 11.28125 51.796875 \r\n",
       "Q 16.75 56 26.8125 56 \r\n",
       "Q 31.78125 56 36.171875 55.265625 \r\n",
       "Q 40.578125 54.546875 44.28125 53.078125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-115\"/>\r\n",
       "      </defs>\r\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\r\n",
       "      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\r\n",
       "      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_13\">\r\n",
       "    <path clip-path=\"url(#p9fb80e292d)\" d=\"M 81.271307 39.213001 \r\n",
       "L 87.35858 37.349773 \r\n",
       "L 90.402216 34.127039 \r\n",
       "L 93.445852 40.071513 \r\n",
       "L 96.489489 49.887998 \r\n",
       "L 99.533125 41.608871 \r\n",
       "L 102.576761 44.39114 \r\n",
       "L 105.620398 43.742225 \r\n",
       "L 108.664034 40.501517 \r\n",
       "L 111.70767 34.852076 \r\n",
       "L 114.751307 36.515801 \r\n",
       "L 117.794943 35.332645 \r\n",
       "L 120.83858 33.062439 \r\n",
       "L 123.882216 32.774379 \r\n",
       "L 126.925852 31.33995 \r\n",
       "L 129.969489 23.554119 \r\n",
       "L 133.013125 20.583039 \r\n",
       "L 136.056761 17.083636 \r\n",
       "L 139.100398 19.663804 \r\n",
       "L 142.144034 20.03362 \r\n",
       "L 145.18767 22.506849 \r\n",
       "L 148.231307 22.091925 \r\n",
       "L 151.274943 19.736184 \r\n",
       "L 154.31858 21.387711 \r\n",
       "L 157.362216 23.609947 \r\n",
       "L 160.405852 24.937859 \r\n",
       "L 163.449489 23.34187 \r\n",
       "L 166.493125 23.731851 \r\n",
       "L 169.536761 24.664075 \r\n",
       "L 172.580398 24.171053 \r\n",
       "L 175.624034 21.430699 \r\n",
       "L 178.66767 20.075989 \r\n",
       "L 181.711307 19.690382 \r\n",
       "L 184.754943 20.502855 \r\n",
       "L 187.79858 25.638258 \r\n",
       "L 190.842216 23.174118 \r\n",
       "L 193.885852 24.554424 \r\n",
       "L 196.929489 23.886774 \r\n",
       "L 199.973125 29.546476 \r\n",
       "L 203.016761 28.49965 \r\n",
       "L 206.060398 29.897105 \r\n",
       "L 209.104034 29.118221 \r\n",
       "L 212.14767 30.077172 \r\n",
       "L 215.191307 29.880144 \r\n",
       "L 218.234943 29.341262 \r\n",
       "L 221.27858 24.975333 \r\n",
       "L 224.322216 22.632508 \r\n",
       "L 227.365852 23.583052 \r\n",
       "L 230.409489 23.705293 \r\n",
       "L 233.453125 26.249764 \r\n",
       "L 236.496761 28.193856 \r\n",
       "L 239.540398 28.443093 \r\n",
       "L 242.584034 28.874437 \r\n",
       "L 245.62767 25.400014 \r\n",
       "L 248.671307 27.160773 \r\n",
       "L 251.714943 26.918646 \r\n",
       "L 254.75858 27.758189 \r\n",
       "L 257.802216 29.171177 \r\n",
       "L 260.845852 28.696698 \r\n",
       "L 263.889489 29.167271 \r\n",
       "L 266.933125 28.300681 \r\n",
       "L 269.976761 28.197678 \r\n",
       "L 273.020398 26.444134 \r\n",
       "L 276.064034 24.466976 \r\n",
       "L 279.10767 23.717886 \r\n",
       "L 282.151307 22.067488 \r\n",
       "L 285.194943 25.032495 \r\n",
       "L 288.23858 27.947929 \r\n",
       "L 291.282216 31.31972 \r\n",
       "L 294.325852 37.068437 \r\n",
       "L 297.369489 49.403843 \r\n",
       "L 300.413125 64.457716 \r\n",
       "L 303.456761 69.883596 \r\n",
       "L 306.500398 78.260232 \r\n",
       "L 309.544034 93.366701 \r\n",
       "L 312.58767 104.788528 \r\n",
       "L 315.631307 113.20816 \r\n",
       "L 318.674943 119.396813 \r\n",
       "L 321.71858 126.987456 \r\n",
       "L 324.762216 134.78618 \r\n",
       "L 327.805852 143.501329 \r\n",
       "L 330.849489 148.755727 \r\n",
       "L 333.893125 161.53634 \r\n",
       "L 336.936761 170.732921 \r\n",
       "L 339.980398 178.154997 \r\n",
       "L 343.024034 185.30571 \r\n",
       "L 346.06767 195.519515 \r\n",
       "L 349.111307 206.245479 \r\n",
       "L 352.154943 214.756364 \r\n",
       "L 355.19858 205.097886 \r\n",
       "L 358.242216 202.767546 \r\n",
       "L 361.285852 199.632563 \r\n",
       "L 364.329489 196.850511 \r\n",
       "L 367.373125 193.329088 \r\n",
       "L 370.416761 190.165311 \r\n",
       "L 373.460398 192.12541 \r\n",
       "L 376.504034 147.623221 \r\n",
       "L 379.54767 144.015377 \r\n",
       "L 382.591307 147.85435 \r\n",
       "L 385.634943 155.213989 \r\n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path d=\"M 66.053125 224.64 \r\n",
       "L 66.053125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path d=\"M 400.853125 224.64 \r\n",
       "L 400.853125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path d=\"M 66.053125 224.64 \r\n",
       "L 400.853125 224.64 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path d=\"M 66.053125 7.2 \r\n",
       "L 400.853125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       " <defs>\r\n",
       "  <clipPath id=\"p9fb80e292d\">\r\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"66.053125\" y=\"7.2\"/>\r\n",
       "  </clipPath>\r\n",
       " </defs>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FaceModel(pl.LightningModule):\n",
    "    def __init__(self, lr = 1e-5):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.example_input_array = torch.rand(1, 3, 64, 64)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 32 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['face'], batch['targets']\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['face'], batch['targets']\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.mse_loss(y_hat, y)\n",
    "        self.log('val_loss', val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch['face'], batch['targets']\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "img_types = ['face']\n",
    "d_train, d_val, d_test = create_datasets(*img_types, batch_size=128)\n",
    "\n",
    "# train\n",
    "logger = TensorBoardLogger(save_dir='logs', name='face', log_graph=True)\n",
    "model = FaceModel()\n",
    "trainer = pl.Trainer(logger=logger, gpus=1, max_epochs=20, benchmark=True, auto_lr_find=True)\n",
    "\n",
    "# find optimal learning rate\n",
    "lr_finder = trainer.tuner.lr_find(model, train_dataloader=d_train)\n",
    "fig = lr_finder.plot(); fig.show()\n",
    "suggested_lr = lr_finder.suggestion()\n",
    "print(\"Suggested LR: {}\".format(suggested_lr))\n",
    "\n",
    "model = FaceModel(lr = suggested_lr)\n",
    "\n",
    "trainer.fit(model, train_dataloader=d_train, val_dataloaders=d_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple input pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 789465.375\n",
      "[1,    11] loss: 686345.602\n",
      "[1,    21] loss: 367307.143\n",
      "[1,    31] loss: 248397.151\n",
      "[1,    41] loss: 188097.015\n",
      "[1,    51] loss: 143288.632\n",
      "[1,    61] loss: 121883.371\n",
      "[1,    71] loss: 108115.594\n",
      "[1,    81] loss: 91716.737\n",
      "[1,    91] loss: 83862.948\n",
      "[1,   101] loss: 74113.728\n",
      "[2,     1] loss: 698187.125\n",
      "[2,    11] loss: 696609.460\n",
      "[2,    21] loss: 359886.777\n",
      "[2,    31] loss: 243040.476\n",
      "[2,    41] loss: 186095.562\n",
      "[2,    51] loss: 148081.926\n",
      "[2,    61] loss: 120961.307\n",
      "[2,    71] loss: 106100.817\n",
      "[2,    81] loss: 90558.151\n",
      "[2,    91] loss: 85122.905\n",
      "[2,   101] loss: 75817.007\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.face_c1 = nn.Conv2d(3, 16, 3)\n",
    "        self.face_p1 = nn.MaxPool2d(2, 2)\n",
    "        self.face_c2 = nn.Conv2d(16, 32, 3)\n",
    "        self.face_p2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.l_eye_c1 = nn.Conv2d(3, 16, 3)\n",
    "        self.l_eye_p1 = nn.MaxPool2d(2, 2)\n",
    "        self.l_eye_c2 = nn.Conv2d(16, 32, 3)\n",
    "        self.l_eye_p2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(12544, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, face, l_eye):\n",
    "        face = self.face_p1(F.relu(self.face_c1(face)))\n",
    "        face = self.face_p2(F.relu(self.face_c2(face)))\n",
    "        face = face.view(-1, 32 * 14 * 14)\n",
    "\n",
    "        l_eye = self.l_eye_p1(F.relu(self.l_eye_c1(l_eye)))\n",
    "        l_eye = self.l_eye_p2(F.relu(self.l_eye_c2(l_eye)))\n",
    "        l_eye = l_eye.view(-1, 32 * 14 * 14)\n",
    "\n",
    "        out = torch.cat((face, l_eye), dim=1)\n",
    "\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train(d_train, d_val, d_test, device):\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "\n",
    "    writer = SummaryWriter('logs/multi/{}'.format(datetime.datetime.now().strftime(\"%m-%d-%Y %H-%M-%S\")))\n",
    "    writer.add_graph(net, [torch.rand(1, 3, 64, 64).to(device), torch.rand(1, 3, 64, 64).to(device)])\n",
    "\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(d_train, 0):\n",
    "            face, l_eye, labels = data['face'].to(device), data['l_eye'].to(device), data['targets'].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(face, l_eye)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 10 == 0:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            writer.add_scalar('Loss/train', loss.item(), i)\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        for i, data in enumerate(d_val, 0):\n",
    "            with torch.no_grad():\n",
    "                face, l_eye, labels = data['face'].to(device), data['l_eye'].to(device), data['targets'].to(device)\n",
    "\n",
    "                outputs = net(face, l_eye)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        writer.add_scalar('Loss/test', val_loss, epoch)\n",
    "\n",
    "\n",
    "# def test(net, d_test, device):\n",
    "#     with torch.no_grad():\n",
    "#         face, l_eye, labels = d_test['face'].to(device), d_test['l_eye'].to(device), d_test['targets'].to(device)\n",
    "\n",
    "#         outputs = net(face, l_eye)\n",
    "\n",
    "#         val_loss = 0\n",
    "#         val_steps = 0\n",
    "#         criterion = nn.MSELoss()\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         val_loss += loss.cpu().numpy()\n",
    "#         val_steps += 1\n",
    "\n",
    "#     return val_loss / val_steps\n",
    "\n",
    "\n",
    "def run():\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "    img_types = ['face', 'l_eye']\n",
    "    d_train, d_val, d_test = create_datasets(*img_types, batch_size=128)\n",
    "\n",
    "    train(d_train, d_val, d_test, device)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple input lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type      | Params | In sizes        | Out sizes      \n",
      "---------------------------------------------------------------------------\n",
      "0 | face_c1  | Conv2d    | 448    | [1, 3, 64, 64]  | [1, 16, 62, 62]\n",
      "1 | face_p1  | MaxPool2d | 0      | [1, 16, 62, 62] | [1, 16, 31, 31]\n",
      "2 | face_c2  | Conv2d    | 4.6 K  | [1, 16, 31, 31] | [1, 32, 29, 29]\n",
      "3 | face_p2  | MaxPool2d | 0      | [1, 32, 29, 29] | [1, 32, 14, 14]\n",
      "4 | l_eye_c1 | Conv2d    | 448    | [1, 3, 64, 64]  | [1, 16, 62, 62]\n",
      "5 | l_eye_p1 | MaxPool2d | 0      | [1, 16, 62, 62] | [1, 16, 31, 31]\n",
      "6 | l_eye_c2 | Conv2d    | 4.6 K  | [1, 16, 31, 31] | [1, 32, 29, 29]\n",
      "7 | l_eye_p2 | MaxPool2d | 0      | [1, 32, 29, 29] | [1, 32, 14, 14]\n",
      "8 | fc1      | Linear    | 401 K  | [1, 12544]      | [1, 32]        \n",
      "9 | fc2      | Linear    | 66     | [1, 32]         | [1, 2]         \n",
      "---------------------------------------------------------------------------\n",
      "411 K     Trainable params\n",
      "0         Non-trainable params\n",
      "411 K     Total params\n",
      "Epoch 0:  89%|████████▉ | 103/116 [00:30<00:03,  3.35it/s, loss=7.65e+05, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0:  91%|█████████ | 105/116 [00:31<00:03,  3.38it/s, loss=7.65e+05, v_num=0]\n",
      "Validating:  15%|█▌        | 2/13 [00:00<00:03,  3.67it/s]\u001b[A\n",
      "Validating:  23%|██▎       | 3/13 [00:00<00:02,  3.58it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 108/116 [00:31<00:02,  3.39it/s, loss=7.65e+05, v_num=0]\n",
      "Validating:  38%|███▊      | 5/13 [00:01<00:02,  3.61it/s]\u001b[A\n",
      "Validating:  46%|████▌     | 6/13 [00:01<00:01,  3.62it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 111/116 [00:32<00:01,  3.39it/s, loss=7.65e+05, v_num=0]\n",
      "Validating:  62%|██████▏   | 8/13 [00:02<00:01,  3.58it/s]\u001b[A\n",
      "Validating:  69%|██████▉   | 9/13 [00:02<00:01,  3.64it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 114/116 [00:33<00:00,  3.40it/s, loss=7.65e+05, v_num=0]\n",
      "Validating:  85%|████████▍ | 11/13 [00:03<00:00,  3.63it/s]\u001b[A\n",
      "Validating:  92%|█████████▏| 12/13 [00:03<00:00,  3.65it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 116/116 [00:34<00:00,  3.38it/s, loss=7.65e+05, v_num=0]\n",
      "Epoch 0: 100%|██████████| 116/116 [00:34<00:00,  3.37it/s, loss=7.65e+05, v_num=0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FaceModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.example_input_array = [torch.rand(1, 3, 64, 64)]*2\n",
    "\n",
    "        self.face_c1 = nn.Conv2d(3, 16, 3)\n",
    "        self.face_p1 = nn.MaxPool2d(2, 2)\n",
    "        self.face_c2 = nn.Conv2d(16, 32, 3)\n",
    "        self.face_p2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.l_eye_c1 = nn.Conv2d(3, 16, 3)\n",
    "        self.l_eye_p1 = nn.MaxPool2d(2, 2)\n",
    "        self.l_eye_c2 = nn.Conv2d(16, 32, 3)\n",
    "        self.l_eye_p2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(12544, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, face, l_eye):\n",
    "        face = self.face_p1(F.relu(self.face_c1(face)))\n",
    "        face = self.face_p2(F.relu(self.face_c2(face)))\n",
    "        face = face.view(-1, 32 * 14 * 14)\n",
    "\n",
    "        l_eye = self.l_eye_p1(F.relu(self.l_eye_c1(l_eye)))\n",
    "        l_eye = self.l_eye_p2(F.relu(self.l_eye_c2(l_eye)))\n",
    "        l_eye = l_eye.view(-1, 32 * 14 * 14)\n",
    "\n",
    "        out = torch.cat((face, l_eye), dim=1)\n",
    "\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        face, l_eye = batch['face'], batch['l_eye']\n",
    "        y_hat = self(face, l_eye)\n",
    "        loss = F.mse_loss(y_hat, batch['targets'])\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch['face'], batch['l_eye'])\n",
    "        val_loss = F.mse_loss(y_hat, batch['targets'])\n",
    "        self.log('val_loss', val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch['face'], batch['l_eye'])\n",
    "        loss = F.mse_loss(y_hat, batch['targets'])\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "img_types = ['face', 'l_eye']\n",
    "d_train, d_val, d_test = create_datasets(*img_types, batch_size=128)\n",
    "\n",
    "# train\n",
    "logger = TensorBoardLogger(save_dir='logs', name='multi/{}'.format(datetime.datetime.now().strftime(\"%m-%d-%Y %H-%M-%S\")), log_graph=True)\n",
    "model = FaceModel()\n",
    "trainer = pl.Trainer(logger=logger, gpus=1, max_epochs=5, benchmark=True)\n",
    "trainer.fit(model, train_dataloader=d_train, val_dataloaders=d_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single - lightning ray tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/31.9 GiB<br>Using AsyncHyperBand: num_stopped=2\n",
       "Bracket: Iter 8.000: -382442.140625 | Iter 4.000: -404997.818359375 | Iter 2.000: -412156.421875 | Iter 1.000: -421098.71875<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/12.16 GiB heap, 0.0/4.2 GiB objects<br>Current best trial: d8c9c_00001 with loss=51721.92578125 and parameters={'batch': 32, 'lr': 0.0037798680153013965, 'layer_1_size': 16}<br>Result logdir: F:\\eyetracker\\logs\\face<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  layer_1_size</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_inner_d8c9c_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   1024</td><td style=\"text-align: right;\">7.21522e-05</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">673987  </td><td style=\"text-align: right;\">                  10</td></tr>\n",
       "<tr><td>_inner_d8c9c_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     32</td><td style=\"text-align: right;\">0.00377987 </td><td style=\"text-align: right;\">            16</td><td style=\"text-align: right;\"> 51721.9</td><td style=\"text-align: right;\">                  10</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-27 10:59:25,784\tINFO tune.py:448 -- Total run time: 340.27 seconds (340.23 seconds for the tuning loop).\n",
      "Best hyperparameters found were:  {'batch': 32, 'lr': 0.0037798680153013965, 'layer_1_size': 16}\n"
     ]
    }
   ],
   "source": [
    "class FaceModel(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.example_input_array = torch.rand(1, 3, 64, 64)\n",
    "        self.layer_1_size = config[\"layer_1_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, self.layer_1_size)\n",
    "        self.fc2 = nn.Linear(self.layer_1_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 32 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['face'], batch['targets']  # needs to be fixed for multi input\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['face'], batch['targets']\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.mse_loss(y_hat, y)\n",
    "        self.log('val_loss', val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch['face'], batch['targets']\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train(config, num_epochs=2, num_gpus=1):\n",
    "    img_types = ['face']\n",
    "    d_train, d_val, d_test = create_datasets(*img_types, batch_size=config[\"batch\"])\n",
    "\n",
    "    model = FaceModel(config)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        gpus=num_gpus,\n",
    "        logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\", log_graph=True),\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=[\n",
    "            TuneReportCallback({\"loss\": \"val_loss\"}, on=\"validation_end\")\n",
    "        ])\n",
    "    trainer.fit(model, train_dataloader=d_train, val_dataloaders=d_val)\n",
    "\n",
    "\n",
    "def dir_name_string(trial):\n",
    "    return str(trial.experiment_tag)\n",
    "\n",
    "\n",
    "def tune_asha(num_samples=2, num_epochs=10, gpus_per_trial=1):\n",
    "    config = {\n",
    "        \"batch\": tune.choice([1 << i for i in range(2, 10)]),\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "        \"layer_1_size\": tune.choice([16, 32, 64]),\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "        parameter_columns=list(config.keys()),\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train,\n",
    "            num_epochs=num_epochs,\n",
    "            num_gpus=gpus_per_trial),\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 1,\n",
    "            \"gpu\": gpus_per_trial\n",
    "        },\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"face\",\n",
    "        trial_dirname_creator=dir_name_string,\n",
    "        local_dir=\"F:/eyetracker/logs\")\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "tune_asha()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple lightning ray tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/31.9 GiB<br>Using AsyncHyperBand: num_stopped=20\n",
       "Bracket: Iter 16.000: -10535.79345703125 | Iter 8.000: -11672.673828125 | Iter 4.000: -17377.427734375 | Iter 2.000: -29676.740234375 | Iter 1.000: -135727.921875<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/12.01 GiB heap, 0.0/4.15 GiB objects<br>Current best trial: 54009_00007 with loss=3586.51318359375 and parameters={'batch': 4, 'lr': 0.013770023564889091, 'layer_1_size': 64}<br>Result logdir: F:\\eyetracker\\logs\\multi<br>Number of trials: 20/20 (20 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  layer_1_size</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_inner_54009_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    128</td><td style=\"text-align: right;\">0.00458689 </td><td style=\"text-align: right;\">            16</td><td style=\"text-align: right;\"> 12179.3 </td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>_inner_54009_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     64</td><td style=\"text-align: right;\">0.00175577 </td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">  8404.33</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>_inner_54009_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">3.59732e-05</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">141292   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    512</td><td style=\"text-align: right;\">1.36201e-05</td><td style=\"text-align: right;\">            16</td><td style=\"text-align: right;\">758800   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0377513  </td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\"> 14818.4 </td><td style=\"text-align: right;\">                  16</td></tr>\n",
       "<tr><td>_inner_54009_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    256</td><td style=\"text-align: right;\">0.00708851 </td><td style=\"text-align: right;\">            16</td><td style=\"text-align: right;\">130869   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    256</td><td style=\"text-align: right;\">0.0190519  </td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">140587   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.01377    </td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">  3586.51</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>_inner_54009_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     16</td><td style=\"text-align: right;\">6.25249e-05</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">152617   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.000304739</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\"> 30158.7 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>_inner_54009_00010</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     32</td><td style=\"text-align: right;\">0.00014527 </td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">149379   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00011</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     64</td><td style=\"text-align: right;\">1.13788e-05</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">746784   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00012</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    512</td><td style=\"text-align: right;\">0.000391383</td><td style=\"text-align: right;\">            16</td><td style=\"text-align: right;\">717617   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00013</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.000312597</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\"> 20753.3 </td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>_inner_54009_00014</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     64</td><td style=\"text-align: right;\">0.0209309  </td><td style=\"text-align: right;\">            16</td><td style=\"text-align: right;\"> 30430.1 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>_inner_54009_00015</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     64</td><td style=\"text-align: right;\">0.000154453</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">159717   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00016</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    128</td><td style=\"text-align: right;\">0.0271443  </td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">  9146.46</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>_inner_54009_00017</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    512</td><td style=\"text-align: right;\">0.000208017</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">735629   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>_inner_54009_00018</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     32</td><td style=\"text-align: right;\">0.0042503  </td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">  4663.6 </td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>_inner_54009_00019</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     64</td><td style=\"text-align: right;\">1.26447e-05</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">755981   </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-27 20:06:35,782\tINFO tune.py:448 -- Total run time: 5046.26 seconds (5046.22 seconds for the tuning loop).\n",
      "Best hyperparameters found were:  {'batch': 4, 'lr': 0.013770023564889091, 'layer_1_size': 64}\n"
     ]
    }
   ],
   "source": [
    "class FaceModel(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.example_input_array = [torch.rand(1, 3, 64, 64)]*2\n",
    "        self.layer_1_size = config[\"layer_1_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "\n",
    "        self.face_c1 = nn.Conv2d(3, 16, 3)\n",
    "        self.face_p1 = nn.MaxPool2d(2, 2)\n",
    "        self.face_c2 = nn.Conv2d(16, 32, 3)\n",
    "        self.face_p2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.l_eye_c1 = nn.Conv2d(3, 16, 3)\n",
    "        self.l_eye_p1 = nn.MaxPool2d(2, 2)\n",
    "        self.l_eye_c2 = nn.Conv2d(16, 32, 3)\n",
    "        self.l_eye_p2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14 * 2, self.layer_1_size)\n",
    "        self.fc2 = nn.Linear(self.layer_1_size, 2)\n",
    "\n",
    "    def forward(self, face, l_eye):\n",
    "        face = self.face_p1(F.relu(self.face_c1(face)))\n",
    "        face = self.face_p2(F.relu(self.face_c2(face)))\n",
    "        face = face.view(-1, 32 * 14 * 14)\n",
    "\n",
    "        l_eye = self.l_eye_p1(F.relu(self.l_eye_c1(l_eye)))\n",
    "        l_eye = self.l_eye_p2(F.relu(self.l_eye_c2(l_eye)))\n",
    "        l_eye = l_eye.view(-1, 32 * 14 * 14)\n",
    "\n",
    "        out = torch.cat((face, l_eye), dim=1)\n",
    "\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch['face'], batch['l_eye'])\n",
    "        loss = F.mse_loss(y_hat, batch['targets'])\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch['face'], batch['l_eye'])\n",
    "        val_loss = F.mse_loss(y_hat, batch['targets'])\n",
    "        self.log('val_loss', val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch['face'], batch['l_eye'])\n",
    "        loss = F.mse_loss(y_hat, batch['targets'])\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train(config, num_epochs=2, num_gpus=1):\n",
    "    img_types = ['face', 'l_eye']\n",
    "    d_train, d_val, d_test = create_datasets(*img_types, batch_size=config[\"batch\"])\n",
    "\n",
    "    model = FaceModel(config)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        gpus=num_gpus,\n",
    "        logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\", log_graph=True),\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=[\n",
    "            TuneReportCallback({\"loss\": \"val_loss\"}, on=\"validation_end\")\n",
    "        ])\n",
    "    trainer.fit(model, train_dataloader=d_train, val_dataloaders=d_val)\n",
    "\n",
    "\n",
    "def dir_name_string(trial):\n",
    "    return str(trial.experiment_tag)\n",
    "\n",
    "\n",
    "def tune_asha(num_samples=20, num_epochs=20, gpus_per_trial=1):\n",
    "    config = {\n",
    "        \"batch\": tune.choice([1 << i for i in range(2, 9)]),\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "        \"layer_1_size\": tune.choice([16, 32, 64]),\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "        parameter_columns=list(config.keys()),\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train,\n",
    "            num_epochs=num_epochs,\n",
    "            num_gpus=gpus_per_trial),\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 1,\n",
    "            \"gpu\": gpus_per_trial\n",
    "        },\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"multi\",\n",
    "        trial_dirname_creator=dir_name_string,\n",
    "        local_dir=\"F:/eyetracker/logs\")\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "tune_asha()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
